{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7729f4b",
   "metadata": {},
   "source": [
    "# Import\n",
    "- 2024-05-18\n",
    "- Language check on Getty ULAN data acquired via SPARQL-endpoint\n",
    "- V. Martens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc987dd6",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6526eb61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:44:37.559473Z",
     "start_time": "2024-05-31T06:44:35.297415Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handling jsons\n",
    "import json\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# spell check module\n",
    "import language_tool_python\n",
    "\n",
    "# creating time stamps\n",
    "# from datetime import datetime\n",
    "import time\n",
    "\n",
    "# importing files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# regex module\n",
    "import re\n",
    "\n",
    "# for multi-threading\n",
    "from concurrent import futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "\n",
    "# back up files\n",
    "import pickle\n",
    "\n",
    "# preferences\n",
    "# adjust pandas to show all cols\n",
    "# pd.set_option('display.max_colwidth', None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397d7e4",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be21c169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:44:37.627130Z",
     "start_time": "2024-05-31T06:44:37.570690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12983, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load lod-query from getty_ulan_sparql_endpoint script\n",
    "%store -r results_df\n",
    "\n",
    "# to create a more standardized naming\n",
    "df_results = results_df.copy()\n",
    "\n",
    "results_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699844e2",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e15a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:44:39.116850Z",
     "start_time": "2024-05-31T06:44:39.110827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240531-084439_df_errors_aat.pickle, 20240531-084439_df_lod_results_aat.pickle, 20240531-084439_found_typos_AAT.xlsx\n"
     ]
    }
   ],
   "source": [
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_df_errors = f'{time_stamp}_df_errors_aat.pickle'\n",
    "\n",
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_df_lod_results = f'{time_stamp}_df_lod_results_aat.pickle'\n",
    "\n",
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_excel_export = f'{time_stamp}_found_typos_AAT.xlsx'\n",
    "\n",
    "print(f\"{filename_df_errors}, {filename_df_lod_results}, {filename_excel_export}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ddfdd",
   "metadata": {},
   "source": [
    "# Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a83d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:45:12.785939Z",
     "start_time": "2024-05-31T06:45:12.771473Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_df_row(df, col, i):\n",
    "    '''\n",
    "    '''\n",
    "    dict_source_matches = {}\n",
    "    try:\n",
    "        matches = tool.check(df[col].iloc[i])\n",
    "        dict_source_matches[df['Subject.value'].iloc[i]] = matches \n",
    "        return dict_source_matches\n",
    "    except (JSONDecodeError, NameError) as e:\n",
    "        return dict_source_matches[df['Subject.value'].iloc[i]] == 'na'\n",
    "        print(e, df['Subject.value'].iloc[i])\n",
    "    \n",
    "    \n",
    "def parse_list_of_jsons(json_list: list) -> pd.DataFrame():\n",
    "    '''doc string'''\n",
    "    df_errors = pd.DataFrame()\n",
    "\n",
    "    for item in json_list:\n",
    "\n",
    "        for k, v in item.items():\n",
    "            source = k \n",
    "\n",
    "            for item in (v):\n",
    "\n",
    "                df_error = pd.DataFrame(\n",
    "                    (\n",
    "                             source,\n",
    "                             item.ruleId,\n",
    "                             item.message,\n",
    "                             item.replacements,\n",
    "                             item.offsetInContext,\n",
    "                             item.context,\n",
    "                             item.offset,\n",
    "                             item.errorLength,\n",
    "                             item.category,\n",
    "                             item.ruleIssueType,\n",
    "                             item.sentence,\n",
    "                             item.context[item.offsetInContext:int(item.offsetInContext + item.errorLength)],\n",
    "                            )\n",
    "                ).T\n",
    "\n",
    "                df_errors = pd.concat([df_errors, df_error])\n",
    "\n",
    "    df_errors = df_errors.rename(columns={\n",
    "              0 : \"url\",\n",
    "              1 : 'ruleId',\n",
    "              2 : 'message', \n",
    "              3 : 'replacements',\n",
    "              4 : 'offsetInContext',\n",
    "              5 : 'context',\n",
    "              6 : 'offset', \n",
    "              7 : 'errorLength',\n",
    "              8 : 'category', \n",
    "              9 : 'ruleIssueType', \n",
    "              10 : 'sentence',\n",
    "              11 : 'misspelledWord'})          \n",
    "\n",
    "    return df_errors\n",
    "\n",
    "def load_latest_file(filepath:str) -> str:\n",
    "    '''args: string with filepath\n",
    "    returns: latest file'''\n",
    "    list_of_files = glob.glob(filepath)\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    return latest_file\n",
    "\n",
    "def create_ulan_weblink(string:str) -> str:\n",
    "    '''from a lod-url creates a regular ulan webpage link\n",
    "    args: ulan lod landing page\n",
    "    returns: regular human readable webpage\n",
    "    '''\n",
    "    ulan_regex = re.compile('\\d+')\n",
    "    match_class_object = re.search(ulan_regex, string)\n",
    "    ulan_id = match_class_object.group(0)\n",
    "    return f\"https://www.getty.edu/vow/ULANFullDisplay?find=&role=&nation=&subjectid={ulan_id}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99bc6d",
   "metadata": {},
   "source": [
    "# Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7458a478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:44:41.026306Z",
     "start_time": "2024-05-31T06:44:41.019331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count = mp.cpu_count()\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e266348e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:44:41.447043Z",
     "start_time": "2024-05-31T06:44:41.423964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10242, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results_df[(results_df['ScopeNote.value'].notnull())]\n",
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe2717d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:44:41.919748Z",
     "start_time": "2024-05-31T06:44:41.902138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject.type</th>\n",
       "      <th>Subject.value</th>\n",
       "      <th>ScopeNote.xml:lang</th>\n",
       "      <th>ScopeNote.type</th>\n",
       "      <th>ScopeNote.value</th>\n",
       "      <th>Term.xml:lang</th>\n",
       "      <th>Term.type</th>\n",
       "      <th>Term.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300189559</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Referring to the sex that in reproduction norm...</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300189557</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Referring to the sex that normally produces eg...</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300451703</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Motility or other conditions that limit a pers...</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>physical disabilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300266528</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Persian wool carpets made in Herat, characteri...</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Herat carpets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300056330</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Pictorial narrative device featuring two or mo...</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>continuous narration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject.type                         Subject.value ScopeNote.xml:lang  \\\n",
       "0          uri  http://vocab.getty.edu/aat/300189559                 en   \n",
       "1          uri  http://vocab.getty.edu/aat/300189557                 en   \n",
       "2          uri  http://vocab.getty.edu/aat/300451703                 en   \n",
       "3          uri  http://vocab.getty.edu/aat/300266528                 en   \n",
       "4          uri  http://vocab.getty.edu/aat/300056330                 en   \n",
       "\n",
       "  ScopeNote.type                                    ScopeNote.value  \\\n",
       "0        literal  Referring to the sex that in reproduction norm...   \n",
       "1        literal  Referring to the sex that normally produces eg...   \n",
       "2        literal  Motility or other conditions that limit a pers...   \n",
       "3        literal  Persian wool carpets made in Herat, characteri...   \n",
       "4        literal  Pictorial narrative device featuring two or mo...   \n",
       "\n",
       "  Term.xml:lang Term.type             Term.value  \n",
       "0            en   literal                   male  \n",
       "1            en   literal                 female  \n",
       "2            en   literal  physical disabilities  \n",
       "3            en   literal          Herat carpets  \n",
       "4            en   literal   continuous narration  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d7f416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T20:44:29.613135Z",
     "start_time": "2024-05-30T20:18:17.550976Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building futures: 100%|███████████████████████████████████████████████████████| 10242/10242 [00:00<00:00, 28062.60it/s]\n",
      "spell check data: 100%|██████████████████████████████████████████████████████████| 10242/10242 [26:08<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 42.7 s\n",
      "Wall time: 26min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# instance of language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Start pool\n",
    "thread_pool = ThreadPoolExecutor(max_workers=cpu_count, thread_name_prefix = 'Thread')\n",
    "\n",
    "# reate futures\n",
    "futures = [thread_pool.submit(check_df_row, results_df, 'ScopeNote.value', i) for i in tqdm(range(len(results_df)), total=len(results_df), desc='building futures')]\n",
    "\n",
    "# submit tasks\n",
    "results = [future.result() for future in tqdm(futures, total=len(futures), desc='spell check data')]\n",
    "\n",
    "# # Changed the func to add a df[col], to make it more compatible\n",
    "# # instance of language_tool_python\n",
    "# tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# # Start pool\n",
    "# thread_pool = ThreadPoolExecutor(max_workers=cpu_count, thread_name_prefix = 'Thread')\n",
    "\n",
    "# # reate futures\n",
    "# futures = [thread_pool.submit(check_df_row, results_df, i) for i in tqdm(range(len(results_df)) , total=len(results_df), desc='building futures')]\n",
    "\n",
    "# # submit tasks\n",
    "# results = [future.result() for future in tqdm(futures, total=len(futures), desc='spell check data')]\n",
    "\n",
    "# results = []\n",
    "# for future in tqdm(futures, total=len(futures), desc='spell check data'):\n",
    "#     try:\n",
    "#         results.append(future.result())\n",
    "#     except (JSONDecodeError, NameError) as e:\n",
    "#         print(e)\n",
    "#         pass\n",
    "\n",
    "# # close tool\n",
    "# tool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aaf996f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T20:45:41.344728Z",
     "start_time": "2024-05-30T20:45:36.437212Z"
    }
   },
   "outputs": [],
   "source": [
    "df_errors = parse_list_of_jsons(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1d7ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T20:45:41.427660Z",
     "start_time": "2024-05-30T20:45:41.427660Z"
    }
   },
   "outputs": [],
   "source": [
    "df_errors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a5f7d",
   "metadata": {},
   "source": [
    "# Back up as a pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555f594",
   "metadata": {},
   "source": [
    "##  Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab78cb23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T20:45:42.352388Z",
     "start_time": "2024-05-30T20:45:42.288834Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f'data_dumps/{filename_df_errors}', 'wb') as handle:\n",
    "    pickle.dump(df_errors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f'data_dumps/{filename_df_lod_results}', 'wb') as handle:\n",
    "    pickle.dump(results_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aad596",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f438edc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:45:19.042604Z",
     "start_time": "2024-05-31T06:45:18.988640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dumps\\20240530-221803_df_lod_results_aat.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10242, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*lod_results_aat.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_lod_results_backup = pickle.load(file)\n",
    "    \n",
    "df_lod_results_backup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9627c006",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:45:20.011458Z",
     "start_time": "2024-05-31T06:45:19.947285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dumps\\20240530-221803_df_errors_aat.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5759, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*errors_aat.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_error_backup = pickle.load(file)\n",
    "    \n",
    "df_error_backup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef395f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:18.087723Z",
     "start_time": "2024-05-25T15:24:18.062755Z"
    }
   },
   "outputs": [],
   "source": [
    "# creates human readable ulan link, based on lod link\n",
    "df_error_backup['url_ulan'] = df_error_backup['url'].apply(create_ulan_weblink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf20f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff1e8c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T20:50:31.765520Z",
     "start_time": "2024-05-30T20:50:31.751777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "misspelledWord\n",
       "cromlech                    1\n",
       "varana                      1\n",
       "ūrdhvacippikā               1\n",
       "roll-moulding               1\n",
       "hypotrachelia               1\n",
       "mikveh                      1\n",
       "Atars                       1\n",
       "balustered                  1\n",
       "rājasēna                    1\n",
       "spandrel                    1\n",
       "bhāraputraka                1\n",
       "gaddi                       1\n",
       "turns                       1\n",
       "Yurok                       1\n",
       "Wiyot                       1\n",
       "Estrangela                  1\n",
       "Serto                       1\n",
       "synchrotron                 1\n",
       "Sakyamuni                   1\n",
       "Orthomyxoviridae            1\n",
       "idaeus                      1\n",
       "recommended to represent    1\n",
       "varous                      1\n",
       "Xiaotun                     1\n",
       "Anyang                      1\n",
       "Yinxu                       1\n",
       "MPEG-4                      1\n",
       "cogged                      1\n",
       "bhūmi-āmalaka               1\n",
       "percipitation               1\n",
       "Franken                     1\n",
       "bhāravāhaka                 1\n",
       "śrīvṛkṣa                    1\n",
       "pōtikā-bracket              1\n",
       "stambhaśīrṣa-bracket        1\n",
       "Jiaozhu                     1\n",
       "Digong                      1\n",
       "taiji                       1\n",
       "kūṭas                       1\n",
       "Time time                   1\n",
       "mandi-type                  1\n",
       "alphasyllabary              1\n",
       "intercrossing               1\n",
       "Nandi                       1\n",
       "kūṭa-niche                  1\n",
       "damage of                   1\n",
       "halakha                     1\n",
       "Titcam                      1\n",
       "Nashat                      1\n",
       "sixty five                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_error_backup['misspelledWord'].value_counts().tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8988491c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T20:47:48.385944Z",
     "start_time": "2024-05-30T20:47:48.372727Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter out punctuation errors\n",
    "df_punctuation = df_error_backup[(df_error_backup['ruleId'] == 'UPPERCASE_SENTENCE_START') | \n",
    "                              (df_error_backup['ruleId'] == 'COMMA_PARENTHESIS_WHITESPACE') |\n",
    "                              (df_error_backup['ruleId'] == 'WHITESPACE_RULE')]\n",
    "\n",
    "df_error_backup = df_error_backup[(df_error_backup['ruleId'] != 'UPPERCASE_SENTENCE_START') &\n",
    "                                  (df_error_backup['ruleId'] != 'COMMA_PARENTHESIS_WHITESPACE') & \n",
    "                                  (df_error_backup['ruleId'] != 'WHITESPACE_RULE')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c1742",
   "metadata": {},
   "source": [
    "# Filter out spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d102956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:19.213713Z",
     "start_time": "2024-05-25T15:24:19.105915Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter out punctuation errors\n",
    "df_punctuation = df_error_backup[(df_error_backup['ruleId'] == 'UPPERCASE_SENTENCE_START') | \n",
    "                              (df_error_backup['ruleId'] == 'COMMA_PARENTHESIS_WHITESPACE') |\n",
    "                              (df_error_backup['ruleId'] == 'WHITESPACE_RULE')]\n",
    "\n",
    "df_error_backup = df_error_backup[(df_error_backup['ruleId'] != 'UPPERCASE_SENTENCE_START') &\n",
    "                                  (df_error_backup['ruleId'] != 'COMMA_PARENTHESIS_WHITESPACE') & \n",
    "                                  (df_error_backup['ruleId'] != 'WHITESPACE_RULE')]\n",
    "\n",
    "# extract checked errors\n",
    "df_checked_errors = df_error_backup[(df_error_backup['misspelledWord'] == 'stained glass') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Brazillian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'in United States') | \n",
    "                                (df_error_backup['misspelledWord'] == 'multi-media ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Britsh') | \n",
    "                                (df_error_backup['misspelledWord'] == 'architec') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Dusseldorf') | \n",
    "                                (df_error_backup['misspelledWord'] == 'architecht') | \n",
    "                                (df_error_backup['misspelledWord'] == 'lanscape') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Ialian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Brazlian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Beligian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'deigner') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Ameican') | \n",
    "                                (df_error_backup['misspelledWord'] == 'archtect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'artistan ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'comtemporary') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cetnury') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Neterlandish') | \n",
    "                                (df_error_backup['misspelledWord'] == 'aarchitect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Kosovan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Oregan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'acitve ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photogapher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Scotish') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cenury') | \n",
    "                                (df_error_backup['misspelledWord'] == 'German born ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'lithpgrapher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'eaqrly') | \n",
    "                                (df_error_backup['misspelledWord'] == ') | ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'dminsitrator') | \n",
    "                                (df_error_backup['misspelledWord'] == 'stuccotist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Amerian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'U.S') | \n",
    "                                (df_error_backup['misspelledWord'] == 'in 1890s') | \n",
    "                                (df_error_backup['misspelledWord'] == 'enameller') | \n",
    "                                (df_error_backup['misspelledWord'] == 'borm') | \n",
    "                                (df_error_backup['misspelledWord'] == 'activeca') | \n",
    "                                (df_error_backup['misspelledWord'] == 'terra-cotta') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Malasian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Spanis') | \n",
    "                                (df_error_backup['misspelledWord'] == 'architet') | \n",
    "                                (df_error_backup['misspelledWord'] == 'veduta') | \n",
    "                                (df_error_backup['misspelledWord'] == 'painterand') | \n",
    "                                (df_error_backup['misspelledWord'] == 'contemporay') | \n",
    "                                (df_error_backup['misspelledWord'] == 'scuptor') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cenutury') | \n",
    "                                (df_error_backup['misspelledWord'] == 'paintier') | \n",
    "                                (df_error_backup['misspelledWord'] == 'mother of pearl ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'tex') | \n",
    "                                (df_error_backup['misspelledWord'] == 'laten') | \n",
    "                                (df_error_backup['misspelledWord'] == 'medallist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Vir') | \n",
    "                                (df_error_backup['misspelledWord'] == 'baptised') | \n",
    "                                (df_error_backup['misspelledWord'] == 'chromolithographer') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Ameerican') | \n",
    "                                (df_error_backup['misspelledWord'] == 'arist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'arist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'actove') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Japenese') | \n",
    "                                (df_error_backup['misspelledWord'] == 'scultptor') | \n",
    "                                (df_error_backup['misspelledWord'] == 'during 1940s') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Brisith') | \n",
    "                                (df_error_backup['misspelledWord'] == 'in 1920s') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cermaicist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'boatbuillder') | \n",
    "                                (df_error_backup['misspelledWord'] == 'nterior') | \n",
    "                                (df_error_backup['misspelledWord'] == 'in 1590s') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Moroccon') | \n",
    "                                (df_error_backup['misspelledWord'] == 'illlustrator') | \n",
    "                                (df_error_backup['misspelledWord'] == 'central Europe') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Belgan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Piemont') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photographert') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Fernch') | \n",
    "                                (df_error_backup['misspelledWord'] == 'aritst') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Austrialian') | \n",
    "                                (df_error_backup['misspelledWord'] == '1680\\'s ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cenutry') | \n",
    "                                (df_error_backup['misspelledWord'] == 'administator') | \n",
    "                                (df_error_backup['misspelledWord'] == 'archtiect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'drafsman') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Canda') | \n",
    "                                (df_error_backup['misspelledWord'] == 'M ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'late late') | \n",
    "                                (df_error_backup['misspelledWord'] == 'jewlery') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photolithographer') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Chiliean') | \n",
    "                                (df_error_backup['misspelledWord'] == '20 century') | \n",
    "                                (df_error_backup['misspelledWord'] == 'comteporary') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Luxumbourgian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Amercian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'horiculturist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Ventian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'glass-blower') | \n",
    "                                (df_error_backup['misspelledWord'] == 'achitect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Argentinine') | \n",
    "                                (df_error_backup['misspelledWord'] == 'lanscapist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Mayasian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'armourer') | \n",
    "                                (df_error_backup['misspelledWord'] == 'counsellor') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Enlgand') | \n",
    "                                (df_error_backup['misspelledWord'] == 'phototgrapher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'paitner') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Hong kong') | \n",
    "                                (df_error_backup['misspelledWord'] == 'jeweller') | \n",
    "                                (df_error_backup['misspelledWord'] == 'bronzeworker') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photograper') | \n",
    "                                (df_error_backup['misspelledWord'] == 'actve') | \n",
    "                                (df_error_backup['misspelledWord'] == 'dutchess ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Berkely') | \n",
    "                                (df_error_backup['misspelledWord'] == 'baptised ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Japansese') | \n",
    "                                (df_error_backup['misspelledWord'] == 'scupltor') | \n",
    "                                (df_error_backup['misspelledWord'] == 'contempory ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'archiect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'copyistr') | \n",
    "                                (df_error_backup['misspelledWord'] == 'active active') | \n",
    "                                (df_error_backup['misspelledWord'] == 'writre') | \n",
    "                                (df_error_backup['misspelledWord'] == 'ironsmith') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Italian Italian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'gardner') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Geman') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Austalian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'paintemaker ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'coppersmith') | \n",
    "                                (df_error_backup['misspelledWord'] == 'architerct') | \n",
    "                                (df_error_backup['misspelledWord'] == 'and and') | \n",
    "                                (df_error_backup['misspelledWord'] == 'copyest') | \n",
    "                                (df_error_backup['misspelledWord'] == 'evironmental ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'articet') | \n",
    "                                (df_error_backup['misspelledWord'] == 'landscsape ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'BagHdad') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Hoston') | \n",
    "                                (df_error_backup['misspelledWord'] == 'craftsperson') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Austrial') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Japanse') | \n",
    "                                (df_error_backup['misspelledWord'] == 'eldest') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Portugese') | \n",
    "                                (df_error_backup['misspelledWord'] == 'intsallation') | \n",
    "                                (df_error_backup['misspelledWord'] == 'amatuer') | \n",
    "                                (df_error_backup['misspelledWord'] == 'archictect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Britian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'draftsmann') | \n",
    "                                (df_error_backup['misspelledWord'] == 'tilemaker') | \n",
    "                                (df_error_backup['misspelledWord'] == 'scenograper') | \n",
    "                                (df_error_backup['misspelledWord'] == 'modelmaker') | \n",
    "                                (df_error_backup['misspelledWord'] == 'anf') | \n",
    "                                (df_error_backup['misspelledWord'] == 'borrn') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photograher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Bazil') | \n",
    "                                (df_error_backup['misspelledWord'] == 'metal worker') | \n",
    "                                (df_error_backup['misspelledWord'] == 'installlation') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photgrapher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'garderner ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Nertherlands') | \n",
    "                                (df_error_backup['misspelledWord'] == 'draftman') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Jamiacan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Michegan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'deisgner') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Dutch born')]\n",
    "\n",
    "print(f\"punctuation: {df_punctuation.shape}, \\n errors_left: {df_error_backup.shape}, \\n checked_errors: {df_checked_errors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38f218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:19.963806Z",
     "start_time": "2024-05-25T15:24:19.770892Z"
    }
   },
   "outputs": [],
   "source": [
    "df_holland = results_df[(results_df['bio.value'].str.contains('Holland') == True)]\n",
    "\n",
    "# creates human readable ulan link, based on lod link\n",
    "df_holland['url_ulan'] = df_holland['x.value'].apply(create_ulan_weblink)\n",
    "\n",
    "# the one errouneous holland\n",
    "df_holland = df_holland[(df_holland['url_ulan'].str.contains('500256837') == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96121ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:20.340798Z",
     "start_time": "2024-05-25T15:24:20.318778Z"
    }
   },
   "outputs": [],
   "source": [
    "# add cols to allign with other cols\n",
    "df_holland['ruleId'] = 'PREFERRED SPELLING'\n",
    "df_holland['message'] = 'Prefered spelling is the Netherlands.'\n",
    "df_holland['replacements'] = ['The Netherlands']\n",
    "df_holland['offsetInContext'] = 0\n",
    "df_holland['context'] = df_holland['bio.value']\n",
    "df_holland['offset'] = 0\n",
    "df_holland['errorLength'] = 0\n",
    "df_holland['category'] = 'PREFERRED SPELLING'\n",
    "df_holland['ruleIssueType'] = 'typographical'\n",
    "df_holland['sentence'] = df_holland['bio.value']\n",
    "df_holland['misspelledWord'] = 'Holland'\n",
    "\n",
    "# rename cols\n",
    "df_holland = df_holland.rename(columns={'x.value':'url'})\n",
    "\n",
    "# select cols\n",
    "df_holland = df_holland[['url', 'ruleId', 'message', 'replacements', 'offsetInContext',\n",
    "       'context', 'offset', 'errorLength', 'category', 'ruleIssueType',\n",
    "       'sentence', 'misspelledWord', 'url_ulan']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb4fb2",
   "metadata": {},
   "source": [
    "# Merge all found errors and create export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98f420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:21.459694Z",
     "start_time": "2024-05-25T15:24:21.446717Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge\n",
    "df_spell_check = pd.concat([df_checked_errors, df_punctuation, df_holland])\n",
    "\n",
    "# select cols\n",
    "df_spell_check = df_spell_check[['url', \n",
    "                                 'url_ulan', \n",
    "                                'sentence',\n",
    "                                 'ruleId', \n",
    "                                 'message', \n",
    "                                 'replacements', \n",
    "                                 'offsetInContext',\n",
    "#                                  'context', \n",
    "                                 'offset', \n",
    "                                 'errorLength', \n",
    "                                 'category', \n",
    "                                 'ruleIssueType',\n",
    "                                 'misspelledWord'\n",
    "                                ]]\n",
    "\n",
    "# rename cols\n",
    "df_spell_check = df_spell_check.rename(columns={'url' : 'url_lod'})\n",
    "\n",
    "# reset index\n",
    "df_spell_check = df_spell_check.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c38b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:22.056206Z",
     "start_time": "2024-05-25T15:24:22.048220Z"
    }
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad58935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:26:54.898285Z",
     "start_time": "2024-05-25T16:26:54.881329Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_excel_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d90477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:29:55.144682Z",
     "start_time": "2024-05-25T16:29:54.699637Z"
    }
   },
   "outputs": [],
   "source": [
    "df_spell_check.to_excel('data_dumps\\\\' + filename_excel_export, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771b49f",
   "metadata": {},
   "source": [
    "# Check for similar names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3130a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:07:56.195951Z",
     "start_time": "2024-05-26T12:07:56.176999Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52631291/vectorizing-or-speeding-up-fuzzywuzzy-string-matching-on-pandas-column\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# df = pd.DataFrame([['cliftonlarsonallen llp minneapolis MN'],\n",
    "#         ['loeb and troper llp newyork NY'],\n",
    "#         [\"dauby o'connor and zaleski llc carmel IN\"],\n",
    "#         ['wegner cpas llp madison WI']],\n",
    "#         columns=['org_name'])\n",
    "\n",
    "name_vals = results_df['name.value'].to_list()\n",
    "\n",
    "# name_vals = name_vals[0:10]\n",
    "\n",
    "threshold = 90\n",
    "\n",
    "def find_match(x):\n",
    "    ''''''\n",
    "    match = process.extract(x, name_vals, limit=2, scorer=fuzz.partial_token_sort_ratio)\n",
    "#     match = match if match[1] > threshold else np.nan\n",
    "    return match\n",
    "\n",
    "# results_df['match_found'] = results_df['name.value'].progress_apply(find_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906590c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T13:13:06.383851Z",
     "start_time": "2024-05-26T13:13:06.357888Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc269d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T13:14:49.292745Z",
     "start_time": "2024-05-26T13:14:47.414889Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# create list\n",
    "name_vals = results_df['name.value'].to_list()\n",
    "\n",
    "name_vals = name_vals[0:5]\n",
    "\n",
    "#Create tuples of brand names, matched brand names, and the score\n",
    "score_sort = [(x,) + i\n",
    "             for x in tqdm(name_vals)\n",
    "             for i in process.extract(x, name_vals, scorer=fuzz.token_sort_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5734263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T13:14:53.582375Z",
     "start_time": "2024-05-26T13:14:53.554703Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a dataframe from the tuples\n",
    "df_similarity = pd.DataFrame(score_sort, columns=['artist','match_sort','similarity_score'])\n",
    "\n",
    "# df_similarity = df_similarity[(df_similarity['score_sort'] > 91) &\n",
    "#                               (df_similarity['score_sort'] != 100)]\n",
    "\n",
    "# # create back up filename for a pickle\n",
    "# time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "# filename_df_similarity = f'{time_stamp}_df_similarity.xlsx'\n",
    "\n",
    "# # export\n",
    "# df_similarity.to_excel('data_dumps\\\\' + filename_df_similarity, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d346e5",
   "metadata": {},
   "source": [
    "# Other options for spell-checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee578f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T07:59:35.947908Z",
     "start_time": "2024-05-26T07:59:35.930455Z"
    }
   },
   "outputs": [],
   "source": [
    "states = ['IA', 'KS', 'UT', 'VA', 'NC', 'NE', 'SD', 'AL', 'ID', 'FM', 'DE', 'AK', 'CT', 'PR', 'NM', 'MS', 'PW', 'CO', 'NJ', 'FL', 'MN', 'VI', 'NV', 'AZ', 'WI', 'ND', 'PA', 'OK', 'KY', 'RI', 'NH', 'MO', 'ME', 'VT', 'GA', 'GU', 'AS', 'NY', 'CA', 'HI', 'IL', 'TN', 'MA', 'OH', 'MD', 'MI', 'WY', 'WA', 'OR', 'MH', 'SC', 'IN', 'LA', 'MP', 'DC', 'MT', 'AR', 'WV', 'TX']\n",
    "regex = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "states2 = ['I.A.', 'K.S.', 'U.T.', 'V.A.', 'N.C.', 'N.E.', 'S.D.', 'A.L.', 'I.D.', 'F.M.', 'D.E.', 'A.K.', 'C.T.',\n",
    "           'P.R.', 'N.M.', 'M.S.', 'P.W.', 'C.O.', 'N.J.', 'F.L.', 'M.N.', 'V.I.', 'N.V.', 'A.Z.', 'W.I.', 'N.D.', \n",
    "           'P.A.', 'O.K.', 'K.Y.', 'R.I.', 'N.H.', 'M.O.', 'M.E.', 'V.T.', 'G.A.', 'G.U.', 'A.S.', 'N.Y.', 'C.A.', \n",
    "           'H.I.', 'I.L.', 'T.N.', 'M.A.', 'O.H.', 'M.D.', 'M.I.', 'W.Y.', 'W.A.', 'O.R.', 'M.H.', 'S.C.', 'I.N.', \n",
    "           'L.A.', 'M.P.', 'D.C.', 'M.T.', 'A.R.', 'W.V.', 'T.X.']\n",
    "regex2 = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672dcb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:14:02.510455Z",
     "start_time": "2024-05-26T08:14:02.494470Z"
    }
   },
   "outputs": [],
   "source": [
    "def state_finder(string:str) -> list:\n",
    "    ''''''\n",
    "    states2 = []\n",
    "#     states = ['IA', 'KS', 'UT', 'VA', 'NC', 'NE', 'SD', 'AL', 'ID', 'FM', 'DE', 'AK', 'CT', 'PR', 'NM', 'MS', 'PW', \n",
    "#               'CO', 'NJ', 'FL', 'MN', 'VI', 'NV', 'AZ', 'WI', 'ND', 'PA', 'OK', 'KY', 'RI', 'NH', 'MO', 'ME', 'VT', \n",
    "#               'GA', 'GU', 'AS', 'NY', 'CA', 'HI', 'IL', 'TN', 'MA', 'OH', 'MD', 'MI', 'WY', 'WA', 'OR', 'MH', 'SC', \n",
    "#               'IN', 'LA', 'MP', 'DC', 'MT', 'AR', 'WV', 'TX']\n",
    "\n",
    "    states = ['I.A.', 'K.S.', 'U.T.', 'V.A.', 'N.C.', 'N.E.', 'S.D.', 'A.L.', 'I.D.', 'F.M.', 'D.E.', 'A.K.', 'C.T.',\n",
    "           'P.R.', 'N.M.', 'M.S.', 'P.W.', 'C.O.', 'N.J.', 'F.L.', 'M.N.', 'V.I.', 'N.V.', 'A.Z.', 'W.I.', 'N.D.', \n",
    "           'P.A.', 'O.K.', 'K.Y.', 'R.I.', 'N.H.', 'M.O.', 'M.E.', 'V.T.', 'G.A.', 'G.U.', 'A.S.', 'N.Y.', 'C.A.', \n",
    "           'H.I.', 'I.L.', 'T.N.', 'M.A.', 'O.H.', 'M.D.', 'M.I.', 'W.Y.', 'W.A.', 'O.R.', 'M.H.', 'S.C.', 'I.N.', \n",
    "           'L.A.', 'M.P.', 'D.C.', 'M.T.', 'A.R.', 'W.V.', 'T.X.']                   \n",
    "                  \n",
    "    regex = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        string = str(string)\n",
    "        states2 = re.findall(regex , string)\n",
    "        return states2\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9ca58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:14:12.445318Z",
     "start_time": "2024-05-26T08:14:05.754724Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tqdm.pandas(desc=\"power DataFrame 1M to 100 random int!\")\n",
    "results_df['test'] = results_df['bio.value'].progress_apply(state_finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7edf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:14:13.336004Z",
     "start_time": "2024-05-26T08:14:13.176084Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[(results_df['test'].str.len() > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6e9f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:22:04.268925Z",
     "start_time": "2024-05-26T08:22:03.498290Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[\n",
    "#     (results_df['bio.value'].str.contains('I.A.') == True) |\n",
    "    (results_df['bio.value'].str.contains('Calif\\.') == True) |\n",
    "    (results_df['bio.value'].str.contains('CA') == True) |\n",
    "    (results_df['bio.value'].str.contains('C\\.A\\.') == True) |\n",
    "    (results_df['bio.value'].str.contains('California') == True)]\n",
    "# (results_df['bio.value'].str.contains('K.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('U.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('S.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('F.M.') == True) |\n",
    "# (results_df['bio.value'].str.contains('D.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.K.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.M.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.W.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.O.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.J.') == True) |\n",
    "# (results_df['bio.value'].str.contains('F.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.V.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.Z.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.K.') == True) |\n",
    "# (results_df['bio.value'].str.contains('K.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('R.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.O.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('G.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('G.U.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('H.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('T.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('S.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('L.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.P.') == True) |\n",
    "# (results_df['bio.value'].str.contains('D.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.V.') == True) |\n",
    "# (results_df['bio.value'].str.contains('T.X.') == True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5a0fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T07:43:33.700146Z",
     "start_time": "2024-05-26T07:43:32.981319Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[\n",
    "#     (results_df\n",
    "#             ['bio.value'].str.contains('Alabama') == True) | \n",
    "# (results_df['bio.value'].str.contains('Alaska') == True) | \n",
    "# (results_df['bio.value'].str.contains('Arizona') == True) | \n",
    "# (results_df['bio.value'].str.contains('Arkansas') == True) | \n",
    "# (results_df['bio.value'].str.contains('California') == True) | \n",
    "# (results_df['bio.value'].str.contains('Colorado') == True) | \n",
    "# (results_df['bio.value'].str.contains('Connecticut') == True) | \n",
    "# (results_df['bio.value'].str.contains('Delaware') == True) | \n",
    "# (results_df['bio.value'].str.contains('Florida') == True) | \n",
    "# (results_df['bio.value'].str.contains('Georgia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Hawaii') == True) | \n",
    "# (results_df['bio.value'].str.contains('Idaho') == True) | \n",
    "# (results_df['bio.value'].str.contains('Illinois') == True) | \n",
    "# (results_df['bio.value'].str.contains('Indiana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Iowa') == True) | \n",
    "# (results_df['bio.value'].str.contains('Kansas') == True) | \n",
    "# (results_df['bio.value'].str.contains('Kentucky') == True) | \n",
    "# (results_df['bio.value'].str.contains('Louisiana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Maine') == True) | \n",
    "# (results_df['bio.value'].str.contains('Maryland') == True) | \n",
    "# (results_df['bio.value'].str.contains('Massachusetts') == True) | \n",
    "# (results_df['bio.value'].str.contains('Michigan') == True) | \n",
    "# (results_df['bio.value'].str.contains('Minnesota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Mississippi') == True) | \n",
    "# (results_df['bio.value'].str.contains('Missouri') == True) | \n",
    "# (results_df['bio.value'].str.contains('Montana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Nebraska') == True) | \n",
    "# (results_df['bio.value'].str.contains('Nevada') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Hampshire') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Jersey') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Mexico') == True) | \n",
    "# (results_df['bio.value'].str.contains('New York') == True) | \n",
    "# (results_df['bio.value'].str.contains('North Carolina') == True) | \n",
    "# (results_df['bio.value'].str.contains('North Dakota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Ohio') == True) | \n",
    "# (results_df['bio.value'].str.contains('Oklahoma') == True) | \n",
    "# (results_df['bio.value'].str.contains('Oregon') == True) | \n",
    "# (results_df['bio.value'].str.contains('Pennsylvania') == True) | \n",
    "# (results_df['bio.value'].str.contains('Rhode Island') == True) | \n",
    "# (results_df['bio.value'].str.contains('South Carolina') == True) | \n",
    "# (results_df['bio.value'].str.contains('South Dakota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Tennessee') == True) | \n",
    "# (results_df['bio.value'].str.contains('Texas') == True) | \n",
    "# (results_df['bio.value'].str.contains('Utah') == True) | \n",
    "# (results_df['bio.value'].str.contains('Vermont') == True) | \n",
    "# (results_df['bio.value'].str.contains('Virginia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Washington') == True) | \n",
    "# (results_df['bio.value'].str.contains('West Virginia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Wisconsin') == True) | \n",
    "# (results_df['bio.value'].str.contains('Wyoming') == True) | \n",
    "# (results_df['bio.value'].str.contains('District of Columbia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Guam') == True) | \n",
    "# (results_df['bio.value'].str.contains('Marshall Islands') == True) | \n",
    "# (results_df['bio.value'].str.contains('Northern Mariana Island') == True) | \n",
    "# (results_df['bio.value'].str.contains('Puerto Rico') == True) | \n",
    "# (results_df['bio.value'].str.contains('Virgin Islands') == True) | \n",
    "# (results_df['bio.value'].str.contains('AL') == True) | \n",
    "# (results_df['bio.value'].str.contains('AK') == True) | \n",
    "# (results_df['bio.value'].str.contains('AZ') == True) | \n",
    "# (results_df['bio.value'].str.contains('AR') == True) | \n",
    "# (results_df['bio.value'].str.contains('CA') == True) | \n",
    "# (results_df['bio.value'].str.contains('CO') == True) | \n",
    "# (results_df['bio.value'].str.contains('CT') == True) | \n",
    "# (results_df['bio.value'].str.contains('DE') == True) | \n",
    "# (results_df['bio.value'].str.contains('FL') == True) | \n",
    "# (results_df['bio.value'].str.contains('GA') == True) | \n",
    "# (results_df['bio.value'].str.contains('HI') == True) | \n",
    "# (results_df['bio.value'].str.contains('ID') == True) | \n",
    "# (results_df['bio.value'].str.contains('IL') == True) | \n",
    "# (results_df['bio.value'].str.contains('IN') == True) | \n",
    "# (results_df['bio.value'].str.contains('IA') == True) | \n",
    "# (results_df['bio.value'].str.contains('KS') == True) | \n",
    "# (results_df['bio.value'].str.contains('KY') == True) | \n",
    "# (results_df['bio.value'].str.contains('LA') == True) | \n",
    "# (results_df['bio.value'].str.contains('ME') == True) | \n",
    "# (results_df['bio.value'].str.contains('MD') == True) | \n",
    "# (results_df['bio.value'].str.contains('MA') == True) | \n",
    "# (results_df['bio.value'].str.contains('MI') == True) | \n",
    "# (results_df['bio.value'].str.contains('MN') == True) | \n",
    "# (results_df['bio.value'].str.contains('MS') == True) | \n",
    "# (results_df['bio.value'].str.contains('MO') == True) | \n",
    "# (results_df['bio.value'].str.contains('MT') == True) | \n",
    "# (results_df['bio.value'].str.contains('NE') == True) | \n",
    "# (results_df['bio.value'].str.contains('NV') == True) | \n",
    "# (results_df['bio.value'].str.contains('NH') == True) | \n",
    "# (results_df['bio.value'].str.contains('NJ') == True) | \n",
    "# (results_df['bio.value'].str.contains('NM') == True) | \n",
    "# (results_df['bio.value'].str.contains('NY') == True) | \n",
    "# (results_df['bio.value'].str.contains('NC') == True) | \n",
    "# (results_df['bio.value'].str.contains('ND') == True) | \n",
    "# (results_df['bio.value'].str.contains('OH') == True) | \n",
    "# (results_df['bio.value'].str.contains('OK') == True) | \n",
    "# (results_df['bio.value'].str.contains('OR') == True) | \n",
    "# (results_df['bio.value'].str.contains('PA') == True) | \n",
    "# (results_df['bio.value'].str.contains('RI') == True) | \n",
    "# (results_df['bio.value'].str.contains('SC') == True) | \n",
    "# (results_df['bio.value'].str.contains('SD') == True) | \n",
    "# (results_df['bio.value'].str.contains('TN') == True) | \n",
    "(results_df['bio.value'].str.contains('TX') == True) | \n",
    "(results_df['bio.value'].str.contains('T.X.') == True) | \n",
    "# (results_df['bio.value'].str.contains('UT') == True) | \n",
    "# (results_df['bio.value'].str.contains('VT') == True) | \n",
    "# (results_df['bio.value'].str.contains('VA') == True) | \n",
    "# (results_df['bio.value'].str.contains('WA') == True) | \n",
    "# (results_df['bio.value'].str.contains('WV') == True) | \n",
    "# (results_df['bio.value'].str.contains('WI') == True) | \n",
    "# (results_df['bio.value'].str.contains('WY') == True) | \n",
    "# (results_df['bio.value'].str.contains('DC') == True) | \n",
    "# (results_df['bio.value'].str.contains('GU') == True) | \n",
    "# (results_df['bio.value'].str.contains('MH') == True) | \n",
    "# (results_df['bio.value'].str.contains('MP') == True) | \n",
    "# (results_df['bio.value'].str.contains('PR') == True) | \n",
    "# (results_df['bio.value'].str.contains('VI') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ala.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Alaska') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ariz.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ark.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Calif.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Color.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Conn.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Del.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Fla.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ga.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Hawaii') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Idaho') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ill.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ind.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Iowa') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Kan.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ky.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' La.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Maine') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Md.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mass.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mich.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Minn.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Miss.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mo.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mont.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Neb.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Nev.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.H.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.J.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.M.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.Y.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.D.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ohio') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Okla.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ore.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Pa.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' R.I.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' S.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' S.Dak.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Tenn.') == True) | \n",
    "(results_df['bio.value'].str.contains(' Tex.') == True)  ]\n",
    "# (results_df['bio.value'].str.contains(' Utah') == True) | \n",
    "# (results_df['bio.value'].str.contains(' V.T.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Va.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wash.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' W.Va.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wis.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wyo.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' D.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Guam') == True) | \n",
    "# (results_df['bio.value'].str.contains(' M.I.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' CNMI') == True) | \n",
    "# (results_df['bio.value'].str.contains(' P.R. or PUR') == True) | \n",
    "# (results_df['bio.value'].str.contains(' V.I.') == True) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283e1aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T07:53:21.650380Z",
     "start_time": "2024-05-26T07:53:20.890701Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df['test'] = results_df['bio.value'].str.extract(r'(?!BC|CE)(A-Z)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2f69f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T07:53:22.259406Z",
     "start_time": "2024-05-26T07:53:22.220232Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[(results_df['test'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b662c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:33:18.075638Z",
     "start_time": "2024-05-20T08:33:17.998025Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[(results_df['bio.value'].str.contains('Tex') == True)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35ec04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:32:53.032231Z",
     "start_time": "2024-05-20T08:32:53.010159Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b28859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T17:12:39.650054Z",
     "start_time": "2024-05-19T17:12:39.636638Z"
    }
   },
   "outputs": [],
   "source": [
    "no_upper['misspelledWord'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695ae29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T16:48:40.530073Z",
     "start_time": "2024-05-19T16:48:40.513517Z"
    }
   },
   "outputs": [],
   "source": [
    "no_upper['misspelledWord'].iloc[26:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec4d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T12:42:54.471239Z",
     "start_time": "2024-05-19T12:42:54.452984Z"
    }
   },
   "outputs": [],
   "source": [
    "df_errors['test'] = df_errors['context'].astype(str).str.extract('([\\s]+-[\\s]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1873bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T12:47:13.861365Z",
     "start_time": "2024-05-19T12:47:13.830450Z"
    }
   },
   "outputs": [],
   "source": [
    "df_errors[(df_errors['test'].notnull())].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel (getty-ulan)",
   "language": "python",
   "name": "getty_ulan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
