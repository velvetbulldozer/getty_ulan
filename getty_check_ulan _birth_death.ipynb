{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ULAN | check active dates\n",
    "- 2024-06-07\n",
    "- Date check on Getty ULAN, data acquired via SPARQL-endpoint\n",
    "- V. Martens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handling jsons\n",
    "import json\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# spell check module\n",
    "import language_tool_python\n",
    "\n",
    "# creating time stamps\n",
    "# from datetime import datetime\n",
    "import time\n",
    "\n",
    "# importing files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# regex module\n",
    "import re\n",
    "\n",
    "# for multi-threading\n",
    "from concurrent import futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "\n",
    "# back up files\n",
    "import pickle\n",
    "\n",
    "# viaf package\n",
    "from viapy import api\n",
    "\n",
    "# preferences\n",
    "# adjust pandas to show all cols\n",
    "# pd.set_option('display.max_colwidth', None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = api.ViafAPI().find_person('vincent van gogh')\n",
    "print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ViafAPI()\n",
    "y = x.find_person('vincent')\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attrmap import attrmap\n",
    "from attrmap import AttrMap\n",
    "import attrmap.utils as au\n",
    "\n",
    "configs = AttrMap(x[3])\n",
    "configs_dict = au.todict(configs)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(configs_dict['recordData']['viafID'])\n",
    "pprint(configs_dict['recordData']['birthDate'])\n",
    "pprint(configs_dict['recordData']['deathDate'])\n",
    "# pprint(configs_dict['recordData']['nationalityOfEntity'])\n",
    "pprint(configs_dict['recordData']['mainHeadings']['data'][1]['text'])\n",
    "\n",
    "# pprint(configs_dict['recordData'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_file(filepath:str) -> str:\n",
    "    '''\n",
    "    loads created latest create file from a directory\n",
    "    args: string with filepath\n",
    "    returns: latest file from a list of files\n",
    "    '''\n",
    "\n",
    "    list_of_files = glob.glob(filepath)\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    \n",
    "    return latest_file\n",
    "\n",
    "def create_ulan_weblink(string:str) -> str:\n",
    "    '''from a lod-url creates a regular ulan webpage link\n",
    "    args: ulan lod landing page\n",
    "    returns: regular human readable webpage\n",
    "    '''\n",
    "    ulan_regex = re.compile('\\d+')\n",
    "    match_class_object = re.search(ulan_regex, string)\n",
    "    ulan_id = match_class_object.group(0)\n",
    "    return f\"https://www.getty.edu/vow/ULANFullDisplay?find=&role=&nation=&subjectid={ulan_id}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_df_errors = f'{time_stamp}_df_errors_TGN.pickle'\n",
    "\n",
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_df_lod_results = f'{time_stamp}_df_lod_results_TGN.pickle'\n",
    "\n",
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_excel_export = f'{time_stamp}_found_inconsistencies_TGN.xlsx'\n",
    "\n",
    "print(f\"{filename_df_errors}, {filename_df_lod_results}, {filename_excel_export}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*ulan.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_ulan = pickle.load(file)\n",
    "\n",
    "df_ulan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ulan.drop(columns=['death.datatype'], inplace=True)\n",
    "for col in df_ulan.columns:\n",
    "    df_ulan.rename(columns={col:col.replace('.value', '')}, inplace=True)\n",
    "    \n",
    "df_ulan['web_link'] = df_ulan['ulan_id'].apply(create_ulan_weblink)\n",
    "\n",
    "df_ulan['nationality'] = df_ulan['nationality'].str.split('(').str[0]\n",
    "df_ulan['type'] = df_ulan['type'].str.split('(').str[0]\n",
    "\n",
    "df_ulan['active'] = df_ulan['bio'].str.extract('(\\d{2}th-\\d{2}th[\\s]+centuries|\\d{4}\\,[\\s]+died[\\s]+\\d{4}|\\d{4}-\\d{4}|\\d{4}-|\\d{4})')\n",
    "df_ulan['active2'] = df_ulan['bio'].str.extract('active(.*\\d{2,}[t][h])')\n",
    "df_ulan['active3'] = df_ulan['bio'].str.extract('(\\d{1,2}th[\\s]+century)')\n",
    "\n",
    "df_ulan['active'] = df_ulan['active'].fillna(df_ulan['active2']).fillna(df_ulan['active3']).str.strip()\n",
    "df_ulan.drop(columns=['active2', 'active3'], inplace=True)\n",
    "\n",
    "unchecked = df_ulan[(df_ulan['active'].isnull()) & (df_ulan['bio'].str.contains('\\d+') == True)]\n",
    "df_ulan = df_ulan[(df_ulan['active'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ulan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "unchecked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# instance of language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Start pool\n",
    "thread_pool = ThreadPoolExecutor(max_workers=cpu_count, thread_name_prefix = 'Thread')\n",
    "\n",
    "# reate futures\n",
    "futures = [thread_pool.submit(check_df_row, results_df, 'ScopeNote.value', i) for i in tqdm(range(len(results_df)), total=len(results_df), desc='building futures')]\n",
    "\n",
    "# submit tasks\n",
    "results = [future.result() for future in tqdm(futures, total=len(futures), desc='spell check data')]\n",
    "\n",
    "# # Changed the func to add a df[col], to make it more compatible\n",
    "# # instance of language_tool_python\n",
    "# tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# # Start pool\n",
    "# thread_pool = ThreadPoolExecutor(max_workers=cpu_count, thread_name_prefix = 'Thread')\n",
    "\n",
    "# # reate futures\n",
    "# futures = [thread_pool.submit(check_df_row, results_df, i) for i in tqdm(range(len(results_df)) , total=len(results_df), desc='building futures')]\n",
    "\n",
    "# # submit tasks\n",
    "# results = [future.result() for future in tqdm(futures, total=len(futures), desc='spell check data')]\n",
    "\n",
    "# results = []\n",
    "# for future in tqdm(futures, total=len(futures), desc='spell check data'):\n",
    "#     try:\n",
    "#         results.append(future.result())\n",
    "#     except (JSONDecodeError, NameError) as e:\n",
    "#         print(e)\n",
    "#         pass\n",
    "\n",
    "# # close tool\n",
    "# tool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors = parse_list_of_jsons(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Back up as a pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "##  Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data_dumps/{filename_df_errors}', 'wb') as handle:\n",
    "    pickle.dump(df_errors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f'data_dumps/{filename_df_lod_results}', 'wb') as handle:\n",
    "    pickle.dump(results_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*lod_results_aat.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_lod_results_backup = pickle.load(file)\n",
    "    \n",
    "df_lod_results_backup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*errors_aat.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_error_backup = pickle.load(file)\n",
    "    \n",
    "df_error_backup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates human readable ulan link, based on lod link\n",
    "df_error_backup['url_ulan'] = df_error_backup['url'].apply(create_ulan_weblink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_backup['misspelledWord'].value_counts().tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out punctuation errors\n",
    "df_punctuation = df_error_backup[(df_error_backup['ruleId'] == 'UPPERCASE_SENTENCE_START') | \n",
    "                              (df_error_backup['ruleId'] == 'COMMA_PARENTHESIS_WHITESPACE') |\n",
    "                              (df_error_backup['ruleId'] == 'WHITESPACE_RULE')]\n",
    "\n",
    "df_error_backup = df_error_backup[(df_error_backup['ruleId'] != 'UPPERCASE_SENTENCE_START') &\n",
    "                                  (df_error_backup['ruleId'] != 'COMMA_PARENTHESIS_WHITESPACE') & \n",
    "                                  (df_error_backup['ruleId'] != 'WHITESPACE_RULE')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# Filter out spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out punctuation errors\n",
    "df_punctuation = df_error_backup[(df_error_backup['ruleId'] == 'UPPERCASE_SENTENCE_START') | \n",
    "                              (df_error_backup['ruleId'] == 'COMMA_PARENTHESIS_WHITESPACE') |\n",
    "                              (df_error_backup['ruleId'] == 'WHITESPACE_RULE')]\n",
    "\n",
    "df_error_backup = df_error_backup[(df_error_backup['ruleId'] != 'UPPERCASE_SENTENCE_START') &\n",
    "                                  (df_error_backup['ruleId'] != 'COMMA_PARENTHESIS_WHITESPACE') & \n",
    "                                  (df_error_backup['ruleId'] != 'WHITESPACE_RULE')]\n",
    "\n",
    "# extract checked errors\n",
    "df_checked_errors = df_error_backup[(df_error_backup['misspelledWord'] == 'stained glass') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Brazillian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'in United States') | \n",
    "                                (df_error_backup['misspelledWord'] == 'multi-media ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Britsh') | \n",
    "                                (df_error_backup['misspelledWord'] == 'architec') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Dusseldorf') | \n",
    "                                (df_error_backup['misspelledWord'] == 'architecht') | \n",
    "                                (df_error_backup['misspelledWord'] == 'lanscape') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Ialian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Brazlian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Beligian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'deigner') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Ameican') | \n",
    "                                (df_error_backup['misspelledWord'] == 'archtect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'artistan ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'comtemporary') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cetnury') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Neterlandish') | \n",
    "                                (df_error_backup['misspelledWord'] == 'aarchitect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Kosovan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Oregan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'acitve ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photogapher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Scotish') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cenury') | \n",
    "                                (df_error_backup['misspelledWord'] == 'German born ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'lithpgrapher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'eaqrly') | \n",
    "                                (df_error_backup['misspelledWord'] == ') | ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'dminsitrator') | \n",
    "                                (df_error_backup['misspelledWord'] == 'stuccotist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Amerian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'U.S') | \n",
    "                                (df_error_backup['misspelledWord'] == 'in 1890s') | \n",
    "                                (df_error_backup['misspelledWord'] == 'enameller') | \n",
    "                                (df_error_backup['misspelledWord'] == 'borm') | \n",
    "                                (df_error_backup['misspelledWord'] == 'activeca') | \n",
    "                                (df_error_backup['misspelledWord'] == 'terra-cotta') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Malasian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Spanis') | \n",
    "                                (df_error_backup['misspelledWord'] == 'architet') | \n",
    "                                (df_error_backup['misspelledWord'] == 'veduta') | \n",
    "                                (df_error_backup['misspelledWord'] == 'painterand') | \n",
    "                                (df_error_backup['misspelledWord'] == 'contemporay') | \n",
    "                                (df_error_backup['misspelledWord'] == 'scuptor') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cenutury') | \n",
    "                                (df_error_backup['misspelledWord'] == 'paintier') | \n",
    "                                (df_error_backup['misspelledWord'] == 'mother of pearl ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'tex') | \n",
    "                                (df_error_backup['misspelledWord'] == 'laten') | \n",
    "                                (df_error_backup['misspelledWord'] == 'medallist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Vir') | \n",
    "                                (df_error_backup['misspelledWord'] == 'baptised') | \n",
    "                                (df_error_backup['misspelledWord'] == 'chromolithographer') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Ameerican') | \n",
    "                                (df_error_backup['misspelledWord'] == 'arist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'arist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'actove') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Japenese') | \n",
    "                                (df_error_backup['misspelledWord'] == 'scultptor') | \n",
    "                                (df_error_backup['misspelledWord'] == 'during 1940s') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Brisith') | \n",
    "                                (df_error_backup['misspelledWord'] == 'in 1920s') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cermaicist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'boatbuillder') | \n",
    "                                (df_error_backup['misspelledWord'] == 'nterior') | \n",
    "                                (df_error_backup['misspelledWord'] == 'in 1590s') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Moroccon') | \n",
    "                                (df_error_backup['misspelledWord'] == 'illlustrator') | \n",
    "                                (df_error_backup['misspelledWord'] == 'central Europe') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Belgan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Piemont') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photographert') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Fernch') | \n",
    "                                (df_error_backup['misspelledWord'] == 'aritst') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Austrialian') | \n",
    "                                (df_error_backup['misspelledWord'] == '1680\\'s ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'cenutry') | \n",
    "                                (df_error_backup['misspelledWord'] == 'administator') | \n",
    "                                (df_error_backup['misspelledWord'] == 'archtiect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'drafsman') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Canda') | \n",
    "                                (df_error_backup['misspelledWord'] == 'M ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'late late') | \n",
    "                                (df_error_backup['misspelledWord'] == 'jewlery') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photolithographer') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Chiliean') | \n",
    "                                (df_error_backup['misspelledWord'] == '20 century') | \n",
    "                                (df_error_backup['misspelledWord'] == 'comteporary') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Luxumbourgian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Amercian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'horiculturist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Ventian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'glass-blower') | \n",
    "                                (df_error_backup['misspelledWord'] == 'achitect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Argentinine') | \n",
    "                                (df_error_backup['misspelledWord'] == 'lanscapist') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Mayasian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'armourer') | \n",
    "                                (df_error_backup['misspelledWord'] == 'counsellor') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Enlgand') | \n",
    "                                (df_error_backup['misspelledWord'] == 'phototgrapher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'paitner') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Hong kong') | \n",
    "                                (df_error_backup['misspelledWord'] == 'jeweller') | \n",
    "                                (df_error_backup['misspelledWord'] == 'bronzeworker') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photograper') | \n",
    "                                (df_error_backup['misspelledWord'] == 'actve') | \n",
    "                                (df_error_backup['misspelledWord'] == 'dutchess ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Berkely') | \n",
    "                                (df_error_backup['misspelledWord'] == 'baptised ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Japansese') | \n",
    "                                (df_error_backup['misspelledWord'] == 'scupltor') | \n",
    "                                (df_error_backup['misspelledWord'] == 'contempory ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'archiect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'copyistr') | \n",
    "                                (df_error_backup['misspelledWord'] == 'active active') | \n",
    "                                (df_error_backup['misspelledWord'] == 'writre') | \n",
    "                                (df_error_backup['misspelledWord'] == 'ironsmith') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Italian Italian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'gardner') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Geman') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Austalian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'paintemaker ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'coppersmith') | \n",
    "                                (df_error_backup['misspelledWord'] == 'architerct') | \n",
    "                                (df_error_backup['misspelledWord'] == 'and and') | \n",
    "                                (df_error_backup['misspelledWord'] == 'copyest') | \n",
    "                                (df_error_backup['misspelledWord'] == 'evironmental ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'articet') | \n",
    "                                (df_error_backup['misspelledWord'] == 'landscsape ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'BagHdad') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Hoston') | \n",
    "                                (df_error_backup['misspelledWord'] == 'craftsperson') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Austrial') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Japanse') | \n",
    "                                (df_error_backup['misspelledWord'] == 'eldest') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Portugese') | \n",
    "                                (df_error_backup['misspelledWord'] == 'intsallation') | \n",
    "                                (df_error_backup['misspelledWord'] == 'amatuer') | \n",
    "                                (df_error_backup['misspelledWord'] == 'archictect') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Britian') | \n",
    "                                (df_error_backup['misspelledWord'] == 'draftsmann') | \n",
    "                                (df_error_backup['misspelledWord'] == 'tilemaker') | \n",
    "                                (df_error_backup['misspelledWord'] == 'scenograper') | \n",
    "                                (df_error_backup['misspelledWord'] == 'modelmaker') | \n",
    "                                (df_error_backup['misspelledWord'] == 'anf') | \n",
    "                                (df_error_backup['misspelledWord'] == 'borrn') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photograher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Bazil') | \n",
    "                                (df_error_backup['misspelledWord'] == 'metal worker') | \n",
    "                                (df_error_backup['misspelledWord'] == 'installlation') | \n",
    "                                (df_error_backup['misspelledWord'] == 'photgrapher') | \n",
    "                                (df_error_backup['misspelledWord'] == 'garderner ') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Nertherlands') | \n",
    "                                (df_error_backup['misspelledWord'] == 'draftman') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Jamiacan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Michegan') | \n",
    "                                (df_error_backup['misspelledWord'] == 'deisgner') | \n",
    "                                (df_error_backup['misspelledWord'] == 'Dutch born')]\n",
    "\n",
    "print(f\"punctuation: {df_punctuation.shape}, \\n errors_left: {df_error_backup.shape}, \\n checked_errors: {df_checked_errors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holland = results_df[(results_df['bio.value'].str.contains('Holland') == True)]\n",
    "\n",
    "# creates human readable ulan link, based on lod link\n",
    "df_holland['url_ulan'] = df_holland['x.value'].apply(create_ulan_weblink)\n",
    "\n",
    "# the one errouneous holland\n",
    "df_holland = df_holland[(df_holland['url_ulan'].str.contains('500256837') == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cols to allign with other cols\n",
    "df_holland['ruleId'] = 'PREFERRED SPELLING'\n",
    "df_holland['message'] = 'Prefered spelling is the Netherlands.'\n",
    "df_holland['replacements'] = ['The Netherlands']\n",
    "df_holland['offsetInContext'] = 0\n",
    "df_holland['context'] = df_holland['bio.value']\n",
    "df_holland['offset'] = 0\n",
    "df_holland['errorLength'] = 0\n",
    "df_holland['category'] = 'PREFERRED SPELLING'\n",
    "df_holland['ruleIssueType'] = 'typographical'\n",
    "df_holland['sentence'] = df_holland['bio.value']\n",
    "df_holland['misspelledWord'] = 'Holland'\n",
    "\n",
    "# rename cols\n",
    "df_holland = df_holland.rename(columns={'x.value':'url'})\n",
    "\n",
    "# select cols\n",
    "df_holland = df_holland[['url', 'ruleId', 'message', 'replacements', 'offsetInContext',\n",
    "       'context', 'offset', 'errorLength', 'category', 'ruleIssueType',\n",
    "       'sentence', 'misspelledWord', 'url_ulan']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Merge all found errors and create export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "df_spell_check = pd.concat([df_checked_errors, df_punctuation, df_holland])\n",
    "\n",
    "# select cols\n",
    "df_spell_check = df_spell_check[['url', \n",
    "                                 'url_ulan', \n",
    "                                'sentence',\n",
    "                                 'ruleId', \n",
    "                                 'message', \n",
    "                                 'replacements', \n",
    "                                 'offsetInContext',\n",
    "#                                  'context', \n",
    "                                 'offset', \n",
    "                                 'errorLength', \n",
    "                                 'category', \n",
    "                                 'ruleIssueType',\n",
    "                                 'misspelledWord'\n",
    "                                ]]\n",
    "\n",
    "# rename cols\n",
    "df_spell_check = df_spell_check.rename(columns={'url' : 'url_lod'})\n",
    "\n",
    "# reset index\n",
    "df_spell_check = df_spell_check.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_excel_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spell_check.to_excel('data_dumps\\\\' + filename_excel_export, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# Check for similar names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52631291/vectorizing-or-speeding-up-fuzzywuzzy-string-matching-on-pandas-column\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# df = pd.DataFrame([['cliftonlarsonallen llp minneapolis MN'],\n",
    "#         ['loeb and troper llp newyork NY'],\n",
    "#         [\"dauby o'connor and zaleski llc carmel IN\"],\n",
    "#         ['wegner cpas llp madison WI']],\n",
    "#         columns=['org_name'])\n",
    "\n",
    "name_vals = results_df['name.value'].to_list()\n",
    "\n",
    "# name_vals = name_vals[0:10]\n",
    "\n",
    "threshold = 90\n",
    "\n",
    "def find_match(x):\n",
    "    ''''''\n",
    "    match = process.extract(x, name_vals, limit=2, scorer=fuzz.partial_token_sort_ratio)\n",
    "#     match = match if match[1] > threshold else np.nan\n",
    "    return match\n",
    "\n",
    "# results_df['match_found'] = results_df['name.value'].progress_apply(find_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# create list\n",
    "name_vals = results_df['name.value'].to_list()\n",
    "\n",
    "name_vals = name_vals[0:5]\n",
    "\n",
    "#Create tuples of brand names, matched brand names, and the score\n",
    "score_sort = [(x,) + i\n",
    "             for x in tqdm(name_vals)\n",
    "             for i in process.extract(x, name_vals, scorer=fuzz.token_sort_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe from the tuples\n",
    "df_similarity = pd.DataFrame(score_sort, columns=['artist','match_sort','similarity_score'])\n",
    "\n",
    "# df_similarity = df_similarity[(df_similarity['score_sort'] > 91) &\n",
    "#                               (df_similarity['score_sort'] != 100)]\n",
    "\n",
    "# # create back up filename for a pickle\n",
    "# time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "# filename_df_similarity = f'{time_stamp}_df_similarity.xlsx'\n",
    "\n",
    "# # export\n",
    "# df_similarity.to_excel('data_dumps\\\\' + filename_df_similarity, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# Other options for spell-checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['IA', 'KS', 'UT', 'VA', 'NC', 'NE', 'SD', 'AL', 'ID', 'FM', 'DE', 'AK', 'CT', 'PR', 'NM', 'MS', 'PW', 'CO', 'NJ', 'FL', 'MN', 'VI', 'NV', 'AZ', 'WI', 'ND', 'PA', 'OK', 'KY', 'RI', 'NH', 'MO', 'ME', 'VT', 'GA', 'GU', 'AS', 'NY', 'CA', 'HI', 'IL', 'TN', 'MA', 'OH', 'MD', 'MI', 'WY', 'WA', 'OR', 'MH', 'SC', 'IN', 'LA', 'MP', 'DC', 'MT', 'AR', 'WV', 'TX']\n",
    "regex = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "states2 = ['I.A.', 'K.S.', 'U.T.', 'V.A.', 'N.C.', 'N.E.', 'S.D.', 'A.L.', 'I.D.', 'F.M.', 'D.E.', 'A.K.', 'C.T.',\n",
    "           'P.R.', 'N.M.', 'M.S.', 'P.W.', 'C.O.', 'N.J.', 'F.L.', 'M.N.', 'V.I.', 'N.V.', 'A.Z.', 'W.I.', 'N.D.', \n",
    "           'P.A.', 'O.K.', 'K.Y.', 'R.I.', 'N.H.', 'M.O.', 'M.E.', 'V.T.', 'G.A.', 'G.U.', 'A.S.', 'N.Y.', 'C.A.', \n",
    "           'H.I.', 'I.L.', 'T.N.', 'M.A.', 'O.H.', 'M.D.', 'M.I.', 'W.Y.', 'W.A.', 'O.R.', 'M.H.', 'S.C.', 'I.N.', \n",
    "           'L.A.', 'M.P.', 'D.C.', 'M.T.', 'A.R.', 'W.V.', 'T.X.']\n",
    "regex2 = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_finder(string:str) -> list:\n",
    "    ''''''\n",
    "    states2 = []\n",
    "#     states = ['IA', 'KS', 'UT', 'VA', 'NC', 'NE', 'SD', 'AL', 'ID', 'FM', 'DE', 'AK', 'CT', 'PR', 'NM', 'MS', 'PW', \n",
    "#               'CO', 'NJ', 'FL', 'MN', 'VI', 'NV', 'AZ', 'WI', 'ND', 'PA', 'OK', 'KY', 'RI', 'NH', 'MO', 'ME', 'VT', \n",
    "#               'GA', 'GU', 'AS', 'NY', 'CA', 'HI', 'IL', 'TN', 'MA', 'OH', 'MD', 'MI', 'WY', 'WA', 'OR', 'MH', 'SC', \n",
    "#               'IN', 'LA', 'MP', 'DC', 'MT', 'AR', 'WV', 'TX']\n",
    "\n",
    "    states = ['I.A.', 'K.S.', 'U.T.', 'V.A.', 'N.C.', 'N.E.', 'S.D.', 'A.L.', 'I.D.', 'F.M.', 'D.E.', 'A.K.', 'C.T.',\n",
    "           'P.R.', 'N.M.', 'M.S.', 'P.W.', 'C.O.', 'N.J.', 'F.L.', 'M.N.', 'V.I.', 'N.V.', 'A.Z.', 'W.I.', 'N.D.', \n",
    "           'P.A.', 'O.K.', 'K.Y.', 'R.I.', 'N.H.', 'M.O.', 'M.E.', 'V.T.', 'G.A.', 'G.U.', 'A.S.', 'N.Y.', 'C.A.', \n",
    "           'H.I.', 'I.L.', 'T.N.', 'M.A.', 'O.H.', 'M.D.', 'M.I.', 'W.Y.', 'W.A.', 'O.R.', 'M.H.', 'S.C.', 'I.N.', \n",
    "           'L.A.', 'M.P.', 'D.C.', 'M.T.', 'A.R.', 'W.V.', 'T.X.']                   \n",
    "                  \n",
    "    regex = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        string = str(string)\n",
    "        states2 = re.findall(regex , string)\n",
    "        return states2\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tqdm.pandas(desc=\"power DataFrame 1M to 100 random int!\")\n",
    "results_df['test'] = results_df['bio.value'].progress_apply(state_finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df['test'].str.len() > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\n",
    "#     (results_df['bio.value'].str.contains('I.A.') == True) |\n",
    "    (results_df['bio.value'].str.contains('Calif\\.') == True) |\n",
    "    (results_df['bio.value'].str.contains('CA') == True) |\n",
    "    (results_df['bio.value'].str.contains('C\\.A\\.') == True) |\n",
    "    (results_df['bio.value'].str.contains('California') == True)]\n",
    "# (results_df['bio.value'].str.contains('K.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('U.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('S.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('F.M.') == True) |\n",
    "# (results_df['bio.value'].str.contains('D.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.K.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.M.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.W.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.O.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.J.') == True) |\n",
    "# (results_df['bio.value'].str.contains('F.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.V.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.Z.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.K.') == True) |\n",
    "# (results_df['bio.value'].str.contains('K.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('R.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.O.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('G.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('G.U.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('H.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('T.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('S.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('L.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.P.') == True) |\n",
    "# (results_df['bio.value'].str.contains('D.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.V.') == True) |\n",
    "# (results_df['bio.value'].str.contains('T.X.') == True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\n",
    "#     (results_df\n",
    "#             ['bio.value'].str.contains('Alabama') == True) | \n",
    "# (results_df['bio.value'].str.contains('Alaska') == True) | \n",
    "# (results_df['bio.value'].str.contains('Arizona') == True) | \n",
    "# (results_df['bio.value'].str.contains('Arkansas') == True) | \n",
    "# (results_df['bio.value'].str.contains('California') == True) | \n",
    "# (results_df['bio.value'].str.contains('Colorado') == True) | \n",
    "# (results_df['bio.value'].str.contains('Connecticut') == True) | \n",
    "# (results_df['bio.value'].str.contains('Delaware') == True) | \n",
    "# (results_df['bio.value'].str.contains('Florida') == True) | \n",
    "# (results_df['bio.value'].str.contains('Georgia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Hawaii') == True) | \n",
    "# (results_df['bio.value'].str.contains('Idaho') == True) | \n",
    "# (results_df['bio.value'].str.contains('Illinois') == True) | \n",
    "# (results_df['bio.value'].str.contains('Indiana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Iowa') == True) | \n",
    "# (results_df['bio.value'].str.contains('Kansas') == True) | \n",
    "# (results_df['bio.value'].str.contains('Kentucky') == True) | \n",
    "# (results_df['bio.value'].str.contains('Louisiana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Maine') == True) | \n",
    "# (results_df['bio.value'].str.contains('Maryland') == True) | \n",
    "# (results_df['bio.value'].str.contains('Massachusetts') == True) | \n",
    "# (results_df['bio.value'].str.contains('Michigan') == True) | \n",
    "# (results_df['bio.value'].str.contains('Minnesota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Mississippi') == True) | \n",
    "# (results_df['bio.value'].str.contains('Missouri') == True) | \n",
    "# (results_df['bio.value'].str.contains('Montana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Nebraska') == True) | \n",
    "# (results_df['bio.value'].str.contains('Nevada') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Hampshire') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Jersey') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Mexico') == True) | \n",
    "# (results_df['bio.value'].str.contains('New York') == True) | \n",
    "# (results_df['bio.value'].str.contains('North Carolina') == True) | \n",
    "# (results_df['bio.value'].str.contains('North Dakota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Ohio') == True) | \n",
    "# (results_df['bio.value'].str.contains('Oklahoma') == True) | \n",
    "# (results_df['bio.value'].str.contains('Oregon') == True) | \n",
    "# (results_df['bio.value'].str.contains('Pennsylvania') == True) | \n",
    "# (results_df['bio.value'].str.contains('Rhode Island') == True) | \n",
    "# (results_df['bio.value'].str.contains('South Carolina') == True) | \n",
    "# (results_df['bio.value'].str.contains('South Dakota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Tennessee') == True) | \n",
    "# (results_df['bio.value'].str.contains('Texas') == True) | \n",
    "# (results_df['bio.value'].str.contains('Utah') == True) | \n",
    "# (results_df['bio.value'].str.contains('Vermont') == True) | \n",
    "# (results_df['bio.value'].str.contains('Virginia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Washington') == True) | \n",
    "# (results_df['bio.value'].str.contains('West Virginia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Wisconsin') == True) | \n",
    "# (results_df['bio.value'].str.contains('Wyoming') == True) | \n",
    "# (results_df['bio.value'].str.contains('District of Columbia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Guam') == True) | \n",
    "# (results_df['bio.value'].str.contains('Marshall Islands') == True) | \n",
    "# (results_df['bio.value'].str.contains('Northern Mariana Island') == True) | \n",
    "# (results_df['bio.value'].str.contains('Puerto Rico') == True) | \n",
    "# (results_df['bio.value'].str.contains('Virgin Islands') == True) | \n",
    "# (results_df['bio.value'].str.contains('AL') == True) | \n",
    "# (results_df['bio.value'].str.contains('AK') == True) | \n",
    "# (results_df['bio.value'].str.contains('AZ') == True) | \n",
    "# (results_df['bio.value'].str.contains('AR') == True) | \n",
    "# (results_df['bio.value'].str.contains('CA') == True) | \n",
    "# (results_df['bio.value'].str.contains('CO') == True) | \n",
    "# (results_df['bio.value'].str.contains('CT') == True) | \n",
    "# (results_df['bio.value'].str.contains('DE') == True) | \n",
    "# (results_df['bio.value'].str.contains('FL') == True) | \n",
    "# (results_df['bio.value'].str.contains('GA') == True) | \n",
    "# (results_df['bio.value'].str.contains('HI') == True) | \n",
    "# (results_df['bio.value'].str.contains('ID') == True) | \n",
    "# (results_df['bio.value'].str.contains('IL') == True) | \n",
    "# (results_df['bio.value'].str.contains('IN') == True) | \n",
    "# (results_df['bio.value'].str.contains('IA') == True) | \n",
    "# (results_df['bio.value'].str.contains('KS') == True) | \n",
    "# (results_df['bio.value'].str.contains('KY') == True) | \n",
    "# (results_df['bio.value'].str.contains('LA') == True) | \n",
    "# (results_df['bio.value'].str.contains('ME') == True) | \n",
    "# (results_df['bio.value'].str.contains('MD') == True) | \n",
    "# (results_df['bio.value'].str.contains('MA') == True) | \n",
    "# (results_df['bio.value'].str.contains('MI') == True) | \n",
    "# (results_df['bio.value'].str.contains('MN') == True) | \n",
    "# (results_df['bio.value'].str.contains('MS') == True) | \n",
    "# (results_df['bio.value'].str.contains('MO') == True) | \n",
    "# (results_df['bio.value'].str.contains('MT') == True) | \n",
    "# (results_df['bio.value'].str.contains('NE') == True) | \n",
    "# (results_df['bio.value'].str.contains('NV') == True) | \n",
    "# (results_df['bio.value'].str.contains('NH') == True) | \n",
    "# (results_df['bio.value'].str.contains('NJ') == True) | \n",
    "# (results_df['bio.value'].str.contains('NM') == True) | \n",
    "# (results_df['bio.value'].str.contains('NY') == True) | \n",
    "# (results_df['bio.value'].str.contains('NC') == True) | \n",
    "# (results_df['bio.value'].str.contains('ND') == True) | \n",
    "# (results_df['bio.value'].str.contains('OH') == True) | \n",
    "# (results_df['bio.value'].str.contains('OK') == True) | \n",
    "# (results_df['bio.value'].str.contains('OR') == True) | \n",
    "# (results_df['bio.value'].str.contains('PA') == True) | \n",
    "# (results_df['bio.value'].str.contains('RI') == True) | \n",
    "# (results_df['bio.value'].str.contains('SC') == True) | \n",
    "# (results_df['bio.value'].str.contains('SD') == True) | \n",
    "# (results_df['bio.value'].str.contains('TN') == True) | \n",
    "(results_df['bio.value'].str.contains('TX') == True) | \n",
    "(results_df['bio.value'].str.contains('T.X.') == True) | \n",
    "# (results_df['bio.value'].str.contains('UT') == True) | \n",
    "# (results_df['bio.value'].str.contains('VT') == True) | \n",
    "# (results_df['bio.value'].str.contains('VA') == True) | \n",
    "# (results_df['bio.value'].str.contains('WA') == True) | \n",
    "# (results_df['bio.value'].str.contains('WV') == True) | \n",
    "# (results_df['bio.value'].str.contains('WI') == True) | \n",
    "# (results_df['bio.value'].str.contains('WY') == True) | \n",
    "# (results_df['bio.value'].str.contains('DC') == True) | \n",
    "# (results_df['bio.value'].str.contains('GU') == True) | \n",
    "# (results_df['bio.value'].str.contains('MH') == True) | \n",
    "# (results_df['bio.value'].str.contains('MP') == True) | \n",
    "# (results_df['bio.value'].str.contains('PR') == True) | \n",
    "# (results_df['bio.value'].str.contains('VI') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ala.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Alaska') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ariz.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ark.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Calif.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Color.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Conn.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Del.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Fla.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ga.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Hawaii') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Idaho') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ill.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ind.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Iowa') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Kan.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ky.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' La.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Maine') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Md.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mass.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mich.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Minn.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Miss.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mo.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mont.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Neb.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Nev.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.H.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.J.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.M.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.Y.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.D.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ohio') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Okla.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ore.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Pa.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' R.I.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' S.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' S.Dak.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Tenn.') == True) | \n",
    "(results_df['bio.value'].str.contains(' Tex.') == True)  ]\n",
    "# (results_df['bio.value'].str.contains(' Utah') == True) | \n",
    "# (results_df['bio.value'].str.contains(' V.T.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Va.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wash.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' W.Va.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wis.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wyo.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' D.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Guam') == True) | \n",
    "# (results_df['bio.value'].str.contains(' M.I.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' CNMI') == True) | \n",
    "# (results_df['bio.value'].str.contains(' P.R. or PUR') == True) | \n",
    "# (results_df['bio.value'].str.contains(' V.I.') == True) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['test'] = results_df['bio.value'].str.extract(r'(?!BC|CE)(A-Z)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df['test'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df['bio.value'].str.contains('Tex') == True)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_upper['misspelledWord'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_upper['misspelledWord'].iloc[26:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors['test'] = df_errors['context'].astype(str).str.extract('([\\s]+-[\\s]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors[(df_errors['test'].notnull())].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getty_ulan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
