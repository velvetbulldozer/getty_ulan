{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AAT data check | 300264086 = Associated Concepts Facet\n",
    "- 2024-05-18\n",
    "- Language check on Getty ULAN data acquired via SPARQL-endpoint\n",
    "- V. Martens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handling jsons\n",
    "import json\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# spell check module\n",
    "import language_tool_python\n",
    "\n",
    "# creating time stamps\n",
    "import time\n",
    "\n",
    "# importing files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# regex module\n",
    "import re\n",
    "\n",
    "# for multi-threading\n",
    "from concurrent import futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# back up files\n",
    "import pickle\n",
    "\n",
    "# check for western spellings\n",
    "from alphabet_detector import AlphabetDetector\n",
    "\n",
    "# spacy lang detect\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "from neg_filter_aat import *\n",
    "\n",
    "# preferences\n",
    "# adjust pandas to show all cols\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### LOD results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lod-query from getty_ulan_sparql_endpoint script\n",
    "%store -r results_df\n",
    "\n",
    "# to create a more standardized naming\n",
    "df_results = results_df.copy()\n",
    "\n",
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_df_errors = f'{time_stamp}_df_errors_aat.pickle'\n",
    "\n",
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_df_lod_results = f'{time_stamp}_df_lod_results_aat.pickle'\n",
    "\n",
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_excel_export = f'{time_stamp}_spell_check_AAT_300264086.xlsx'\n",
    "\n",
    "print(f\"{filename_df_errors}, {filename_df_lod_results}, {filename_excel_export}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory('language_detector')\n",
    "def language_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "def check_df_row(df, col, i):\n",
    "    '''\n",
    "    '''\n",
    "    dict_source_matches = {}\n",
    "    try:\n",
    "        matches = tool.check(df[col].iloc[i])\n",
    "        dict_source_matches[df['Subject.value'].iloc[i]] = matches \n",
    "        return dict_source_matches\n",
    "    except (JSONDecodeError, NameError) as e:\n",
    "        return dict_source_matches[df['Subject.value'].iloc[i]] == 'na'\n",
    "        print(e, df['Subject.value'].iloc[i])\n",
    "    \n",
    "    \n",
    "def parse_list_of_jsons(json_list: list) -> pd.DataFrame():\n",
    "    '''doc string'''\n",
    "    df_errors = pd.DataFrame()\n",
    "\n",
    "    for item in json_list:\n",
    "\n",
    "        for k, v in item.items():\n",
    "            source = k \n",
    "\n",
    "            for item in (v):\n",
    "\n",
    "                df_error = pd.DataFrame(\n",
    "                    (\n",
    "                             source,\n",
    "                             item.ruleId,\n",
    "                             item.message,\n",
    "                             item.replacements,\n",
    "                             item.offsetInContext,\n",
    "                             item.context,\n",
    "                             item.offset,\n",
    "                             item.errorLength,\n",
    "                             item.category,\n",
    "                             item.ruleIssueType,\n",
    "                             item.sentence,\n",
    "                             item.context[item.offsetInContext:int(item.offsetInContext + item.errorLength)],\n",
    "                            )\n",
    "                ).T\n",
    "\n",
    "                df_errors = pd.concat([df_errors, df_error])\n",
    "\n",
    "    df_errors = df_errors.rename(columns={\n",
    "              0 : \"url\",\n",
    "              1 : 'ruleId',\n",
    "              2 : 'message', \n",
    "              3 : 'replacements',\n",
    "              4 : 'offsetInContext',\n",
    "              5 : 'context',\n",
    "              6 : 'offset', \n",
    "              7 : 'errorLength',\n",
    "              8 : 'category', \n",
    "              9 : 'ruleIssueType', \n",
    "              10 : 'sentence',\n",
    "              11 : 'misspelledWord'})          \n",
    "\n",
    "    return df_errors\n",
    "\n",
    "def load_latest_file(filepath:str) -> str:\n",
    "    '''args: string with filepath\n",
    "    returns: latest file'''\n",
    "    list_of_files = glob.glob(filepath)\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    return latest_file\n",
    "\n",
    "def create_aat_weblink(string:str) -> str:\n",
    "    '''from a lod-url creates a regular ulan webpage link\n",
    "    args: ulan lod landing page\n",
    "    returns: regular human readable webpage\n",
    "    '''\n",
    "    aat_regex = re.compile('\\d+')\n",
    "    match_class_object = re.search(aat_regex, string)\n",
    "    aat_id = match_class_object.group(0)\n",
    "    return f\"https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid={aat_id}\"\n",
    "\n",
    "def check_for_western_chars(string):\n",
    "    try:\n",
    "        valids = re.match(r\"[A-Za-z-]+\", string).group(0)\n",
    "        return valids\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def check_for_capitalization(string):\n",
    "    try:\n",
    "        valids = re.match(r\"^[A-Z]\", string).group(0)\n",
    "        return valids\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = mp.cpu_count()\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df[(results_df['ScopeNote.value'].notnull())]\n",
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# instance of language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Start pool\n",
    "thread_pool = ThreadPoolExecutor(max_workers=cpu_count, thread_name_prefix = 'Thread')\n",
    "\n",
    "# reate futures\n",
    "futures = [thread_pool.submit(check_df_row, results_df, 'ScopeNote.value', i) for i in tqdm(range(len(results_df)), total=len(results_df), desc='building futures')]\n",
    "\n",
    "# submit tasks\n",
    "results = [future.result() for future in tqdm(futures, total=len(futures), desc='spell check data')]\n",
    "\n",
    "# # Changed the func to add a df[col], to make it more compatible\n",
    "# # instance of language_tool_python\n",
    "# tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# # Start pool\n",
    "# thread_pool = ThreadPoolExecutor(max_workers=cpu_count, thread_name_prefix = 'Thread')\n",
    "\n",
    "# # reate futures\n",
    "# futures = [thread_pool.submit(check_df_row, results_df, i) for i in tqdm(range(len(results_df)) , total=len(results_df), desc='building futures')]\n",
    "\n",
    "# # submit tasks\n",
    "# results = [future.result() for future in tqdm(futures, total=len(futures), desc='spell check data')]\n",
    "\n",
    "# results = []\n",
    "# for future in tqdm(futures, total=len(futures), desc='spell check data'):\n",
    "#     try:\n",
    "#         results.append(future.result())\n",
    "#     except (JSONDecodeError, NameError) as e:\n",
    "#         print(e)\n",
    "#         pass\n",
    "\n",
    "# # close tool\n",
    "# tool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors = parse_list_of_jsons(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Back up as a pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "##  Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data_dumps/{filename_df_errors}', 'wb') as handle:\n",
    "    pickle.dump(df_errors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f'data_dumps/{filename_df_lod_results}', 'wb') as handle:\n",
    "    pickle.dump(results_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*lod_results_aat.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_lod_results_backup = pickle.load(file)\n",
    "    \n",
    "df_lod_results_backup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*errors_aat.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_error_backup = pickle.load(file)\n",
    "    \n",
    "df_error_backup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates human readable ulan link, based on lod link\n",
    "df_error_backup['url_aat'] = df_error_backup['url'].apply(create_aat_weblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Filter out spelling mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Subsetting based on ruleId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out punctuation errors\n",
    "df_punctuation = df_error_backup[(df_error_backup['ruleId'] == 'UPPERCASE_SENTENCE_START') | \n",
    "                                 (df_error_backup['ruleId'] == 'COMMA_PARENTHESIS_WHITESPACE') |\n",
    "                                 (df_error_backup['ruleId'] == 'WHITESPACE_RULE') | \n",
    "                                 (df_error_backup['ruleId'].str.contains('EN_COMPOUNDS') == True) | \n",
    "                                 (df_error_backup['ruleId'].str.contains('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA') == True) | \n",
    "                                 (df_error_backup['ruleId'].str.contains('MISSING_COMMA_AFTER_INTRODUCTORY_PHRASE') == True) | \n",
    "                                 (df_error_backup['ruleId'] == 'SENTENCE_WHITESPACE' ) | \n",
    "                                 (df_error_backup['ruleId'] == 'ETC_PERIOD') | \n",
    "                                 (df_error_backup['ruleId'] == 'LC_AFTER_PERIOD') | # punctuation\n",
    "                                 (df_error_backup['ruleId'] == 'EN_DIACRITICS_REPLACE_DECOLLETAGE') | # \n",
    "                                 (df_error_backup['ruleId'] == 'EN_DIACRITICS_REPLACE_CREPE') |  # punctuation\n",
    "                                 (df_error_backup['ruleId'] == 'FILE_EXTENSIONS_CASE')] # caps\n",
    "\n",
    "\n",
    "df_agreement = df_error_backup[((df_error_backup['ruleId'] == 'AGREEMENT_SENT_START') & (df_error_backup['misspelledWord'] == 'remains')) |\n",
    "                               ((df_error_backup['ruleId'] == 'AGREEMENT_SENT_START') & (df_error_backup['misspelledWord'] == 'locks')) |\n",
    "                               ((df_error_backup['ruleId'] == 'AGREEMENT_SENT_START') & (df_error_backup['misspelledWord'] == 'is'))]\n",
    "\n",
    "# filter out grammatical errors\n",
    "df_grammar = df_error_backup[(df_error_backup['ruleId'].str.contains('DEPEND_ON') == True) |\n",
    "                             (df_error_backup['ruleId'].str.contains('FEWER_LESS') == True) |\n",
    "                             (df_error_backup['ruleId'].str.contains('ENGLISH_WORD_REPEAT_BEGINNING_RULE') == True) |\n",
    "                             (df_error_backup['ruleId'].str.contains('EN_DIACRITICS_REPLACE_CREPE') == True) |\n",
    "                             (df_error_backup['ruleId'] == 'FOR_FRO') |\n",
    "                             (df_error_backup['ruleId'] == 'SUBJECT_VERB_AGREEMENT_PLURAL') |\n",
    "                             (df_error_backup['ruleId'] == 'NODT_DOZEN') |\n",
    "                             (df_error_backup['ruleId'] == 'TO_NON_BASE') |\n",
    "                             (df_error_backup['ruleId'] == 'AS_IS_VBG') |\n",
    "                             (df_error_backup['ruleId'] == 'MD_BASEFORM') |\n",
    "                             (df_error_backup['ruleId'] == 'THROUGH_THOROUGH') | # not a false hit, but needs to be throughflow, instead of 'through flow'\n",
    "                             (df_error_backup['ruleId'] == 'MANY_KINDS_OF') |\n",
    "                             (df_error_backup['ruleId'] == 'BE_VBP_IN') |\n",
    "                             (df_error_backup['ruleId'] == 'AFTERWARDS_US') |\n",
    "                             (df_error_backup['ruleId'] == 'NNS_THAT_VBZ') |\n",
    "                             (df_error_backup['ruleId'] == 'PLURAL_MODIFIER') |\n",
    "                             (df_error_backup['ruleId'] == 'PRP_PAST_PART') |\n",
    "                             (df_error_backup['ruleId'] == 'EVERYDAY_EVERY_DAY') |\n",
    "                             (df_error_backup['ruleId'] == 'ENGLISH_WORD_REPEAT_BEGINNING_RULE') |\n",
    "                             (df_error_backup['ruleId'] == 'FOR_FRO') |\n",
    "                             (df_error_backup['ruleId'] == 'SUBJECT_VERB_AGREEMENT_PLURAL') |\n",
    "                             (df_error_backup['ruleId'] == 'ARTICLE_ADJECTIVE_OF') |\n",
    "                             (df_error_backup['ruleId'] == 'EN_A_VS_AN') |\n",
    "                             (df_error_backup['ruleId'] == 'POSSESSIVE_APOSTROPHE') |\n",
    "                             (df_error_backup['ruleId'] == 'THE_SUPERLATIVE') |\n",
    "                             (df_error_backup['ruleId'] == 'PLURAL_THAT_AGREEMENT')]\n",
    "\n",
    "# subsetting on false hits, and historical left-over\n",
    "df_error_backup = df_error_backup[(df_error_backup['ruleId'] != 'UPPERCASE_SENTENCE_START') &\n",
    "                                  (df_error_backup['ruleId'] != 'COMMA_PARENTHESIS_WHITESPACE') & \n",
    "                                  (df_error_backup['ruleId'] != 'WHITESPACE_RULE') &\n",
    "                                  (df_error_backup['ruleId'].str.contains('EN_COMPOUNDS') == False) & \n",
    "                                  (df_error_backup['ruleId'].str.contains('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA') == False) &\n",
    "                                  (df_error_backup['ruleId'].str.contains('MISSING_COMMA_AFTER_INTRODUCTORY_PHRASE') == False) & \n",
    "                                  (df_error_backup['ruleId'] != 'SENTENCE_WHITESPACE' ) &\n",
    "                                  (df_error_backup['ruleId'].str.contains('DEPEND_ON') == False) &\n",
    "                                  (df_error_backup['ruleId'] != 'ETC_PERIOD') & \n",
    "                                  (df_error_backup['ruleId'] != 'LC_AFTER_PERIOD') & # punctuation\n",
    "                                  (df_error_backup['ruleId'] != 'EN_DIACRITICS_REPLACE_DECOLLETAGE') & # punctuation\n",
    "                                  (df_error_backup['ruleId'] != 'EN_DIACRITICS_REPLACE_CREPE') & # punctuation\n",
    "                                  (df_error_backup['ruleId'] != 'FILE_EXTENSIONS_CASE') & # caps\n",
    "                                  (df_error_backup['ruleId'].str.contains('RED_READ') == False) & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'RITE_WRITE') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'FROM_FORM') &  # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'BESIDES_BESIDE') &  # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'NEITHER_NOR') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'OTHER_OTHERS') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'ITS_TO_IT_S') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'IT_IS_JJ_TO_VBG') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'CANT') & \n",
    "                                  (df_error_backup['ruleId'] != 'AGREEMENT_SENT_START' )] # false hit\n",
    "\n",
    "# randomly found errors\n",
    "df_random_finds =  df_error_backup[(df_error_backup['misspelledWord'] == 'Scandanavian') |\n",
    "                                   (df_error_backup['misspelledWord'] == 'Piemontese')]\n",
    "\n",
    "df_floor = df_error_backup[(df_error_backup['misspelledWord'] == 'storey' ) |  # floor in us english\n",
    "                           (df_error_backup['misspelledWord'] == 'storeys' )]\n",
    "\n",
    "\n",
    "# if ruleId not in df_other_errors and not in df_random_finds\n",
    "df_error_backup = df_error_backup[(~df_error_backup['ruleId'].isin(df_grammar['ruleId'])) & \n",
    "                                  (~df_error_backup['ruleId'].isin(df_punctuation['ruleId']))]\n",
    "\n",
    "df_error_backup = df_error_backup[(~df_error_backup['misspelledWord'].isin(df_random_finds['misspelledWord']))]\n",
    "\n",
    "print(f\"grammar mistakes: {df_grammar.shape[0]}\\n agreement errors: {df_agreement.shape[0]} \\n punctuation mistakes {df_punctuation.shape[0]} \\n other errors: {df_error_backup.shape[0]} \\n floors: {df_floor.shape[0]} \\n random finds: {df_random_finds.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Filter out foreign hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters out sentences with most likely foreign words \n",
    "df_foreign = df_error_backup[(df_error_backup['sentence'].str.lower().str.contains('indian') == True) |\n",
    "                             (df_error_backup['sentence'].str.lower().str.contains('arabic') == True) |\n",
    "                             (df_error_backup['sentence'].str.lower().str.contains('chinese') == True) |\n",
    "                             (df_error_backup['sentence'].str.lower().str.contains('persian') == True) |\n",
    "                             (df_error_backup['sentence'].str.lower().str.contains('nahuatl') == True) ]\n",
    "\n",
    "# checked subset for hits and subset those into a df\n",
    "df_foreign_positive = df_foreign[(df_foreign['misspelledWord'].str.contains('mould') == True) | \n",
    "                                 (df_foreign['misspelledWord'].str.contains('storey') == True) |\n",
    "                                 (df_foreign['misspelledWord'] == 'doorframe') |\n",
    "                                 (df_foreign['misspelledWord'] == 'coffered') |\n",
    "                                 (df_foreign['misspelledWord'] == 'doorsill') |\n",
    "                                 (df_foreign['misspelledWord'] == 'oversailing') |\n",
    "                                 (df_foreign['misspelledWord'] == 'atlantid') |\n",
    "                                 (df_foreign['misspelledWord'] == '\\'')]\n",
    "\n",
    "# filters out sentences with most likely foreign words\n",
    "df_error_backup = df_error_backup[(df_error_backup['sentence'].str.lower().str.contains('indian') == False) & \n",
    "                                  (df_error_backup['sentence'].str.lower().str.contains('arabic') == False) &\n",
    "                                  (df_error_backup['sentence'].str.lower().str.contains('chinese') == False) &\n",
    "                                  (df_error_backup['sentence'].str.lower().str.contains('persian') == False) & \n",
    "                                  (df_error_backup['sentence'].str.lower().str.contains('nahuatl') == False)]\n",
    "\n",
    "print(f\"other errors: {df_error_backup.shape[0]}, \\n foreign errors: {df_foreign_positive.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Filter out frequently occuring mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_backup = negative_aat_filter(df_error_backup)\n",
    "print(f\"other errors: {df_error_backup.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Filter out words that start with a capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run regex func\n",
    "df_error_backup['capital_start'] = df_error_backup['misspelledWord'].apply(check_for_capitalization)\n",
    "\n",
    "df_capitals = df_error_backup[(df_error_backup['capital_start'].notnull())]\n",
    "\n",
    "df_capitals_positive = df_capitals[(df_capitals['misspelledWord'] == 'Bookbindings' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'Web site' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'Catholic church' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'Eastern Orthodox church' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'It dark' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'With the exception of' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'Finno-Ugric' ) | \n",
    "                                   (df_capitals['misspelledWord'] == 'Syrian desert' ) ]\n",
    "\n",
    "df_error_backup = df_error_backup[(df_error_backup['capital_start'].isnull())]\n",
    "del df_error_backup['capital_start']\n",
    "\n",
    "print(f\"other errors: {df_error_backup.shape[0]}, \\n {df_capitals_positive.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Check for language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsets chars from non western chars\n",
    "df_error_backup['western_chars'] = df_error_backup['misspelledWord'].apply(check_for_western_chars)\n",
    "\n",
    "# see if length is equal otherwise drop non-western chars\n",
    "df_error_backup = df_error_backup[(df_error_backup['western_chars'].str.len() == df_error_backup['misspelledWord'].str.len())]\n",
    "\n",
    "print(f\"other errors: {df_error_backup.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Check for non-Western typescripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AlphabetDetector()\n",
    "df_error_backup['western_typo'] = df_error_backup['misspelledWord'].apply(lambda x: ad.is_latin(x))\n",
    "\n",
    "print(f\"non-western chars: {df_error_backup[(df_error_backup['western_typo'] == False )].shape[0]}\")\n",
    "df_error_backup = df_error_backup[(df_error_backup['western_typo'] == True )]\n",
    "\n",
    "del df_error_backup['western_typo']\n",
    "\n",
    "print(f\"other errors: {df_error_backup.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = pd.DataFrame(df_error_backup['misspelledWord'].value_counts().iloc[95:100]).reset_index()\n",
    "# list_error = x['misspelledWord'].to_list()\n",
    "# print(list_error)\n",
    "\n",
    "# df_error_backup[(df_error_backup['misspelledWord'].isin(list_error))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in manual:\n",
    "    # print(f\"                (df_error_backup['misspelledWord'] != '{item}' ) &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Lang detect Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 2000000\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "# initiate master df\n",
    "df_spacies = pd.DataFrame()\n",
    "\n",
    "# run over words\n",
    "for item in tqdm(df_error_backup['misspelledWord'].tolist()):\n",
    "    \n",
    "    # store data in doc\n",
    "    doc = nlp(str(item))\n",
    "    detect_language = doc._.language\n",
    "\n",
    "    # cast and store results into variable\n",
    "    word = str(doc)\n",
    "    language = str(detect_language['language'])\n",
    "    score =  float(detect_language['score'])\n",
    "\n",
    "    # add to df\n",
    "    df_spacy = pd.DataFrame((word, language, score)).T\n",
    "    df_spacies = pd.concat([df_spacy, df_spacies])\n",
    "\n",
    "# manipulate all spacy results\n",
    "df_spacies = (df_spacies\n",
    "             .rename(columns={0: 'word',\n",
    "                              1: 'language',\n",
    "                              2: 'score'})\n",
    "             .sort_values('score', ascending=False)\n",
    "             .groupby(by=['word', 'language'])\n",
    "             .first()\n",
    "             .reset_index())\n",
    "\n",
    "# subset false hits\n",
    "df_spacies = df_spacies[(df_spacies['word'].str.len() > 3)]\n",
    "\n",
    "# merge back with main df\n",
    "df_error_backup = pd.merge(df_error_backup, \n",
    "                           df_spacies, \n",
    "                           left_on='misspelledWord', \n",
    "                           right_on='word', \n",
    "                           how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# Check for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run spacy on trained name data\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "entity_list = []\n",
    "\n",
    "# run over annotations\n",
    "for i in tqdm(df_error_backup['misspelledWord'].tolist()):\n",
    "    # store data in doc\n",
    "    doc = nlp(str(i))\n",
    "    \n",
    "    # retrieve entities from doc and ad it to a list\n",
    "    for entity in doc.ents:\n",
    "#         print(entity.text, entity.label_)\n",
    "        entity_list.append((i, entity.text, entity.label_))\n",
    "    \n",
    "# untuple the list    \n",
    "untupled = pd.DataFrame([[y for y in  x] for x in entity_list])\n",
    "\n",
    "# add col names\n",
    "untupled = untupled.rename(columns={0:'word',\n",
    "                                    1:'spacy_word',\n",
    "                                    2:'entity_label'})\n",
    "\n",
    "# merge back with main df\n",
    "df_error_backup = pd.merge(df_error_backup, \n",
    "                           untupled, \n",
    "                           left_on='misspelledWord', \n",
    "                           right_on='word', \n",
    "                           how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# Merge all errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "df_spell_check = pd.concat([df_agreement, df_error_backup, df_grammar, df_punctuation, df_random_finds, df_floor, df_foreign_positive, df_capitals_positive])\n",
    "\n",
    "# select cols\n",
    "df_spell_check = df_spell_check[['url', \n",
    "                                 'url_aat', \n",
    "                                 'ruleId', \n",
    "                                 'message', \n",
    "                                 'sentence',\n",
    "                                 'misspelledWord',\n",
    "                                 'replacements', \n",
    "                                #  'offsetInContext',\n",
    "#                                  'context', \n",
    "                                 'offset', \n",
    "                                 'errorLength', \n",
    "                                 'category', \n",
    "                                 'ruleIssueType',\n",
    "                                ]]\n",
    "\n",
    "# df_spell_check.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "# rename cols\n",
    "df_spell_check = df_spell_check.rename(columns={'url' : 'url_lod_aat'})\n",
    "\n",
    "# reset index\n",
    "df_spell_check = df_spell_check.reset_index().drop(columns='index')\n",
    "\n",
    "print(f\"errors found: {df_spell_check.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spell_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spell_check.to_excel('data_dumps\\\\' + filename_excel_export, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doubles = df_spell_check.groupby(['url_aat', 'offsetInContext', 'misspelledWord']).size().reset_index().rename(columns={0 : 'freq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(df_spell_check, df_doubles, on=['url_aat', 'misspelledWord'], how='left')\n",
    "test['freq'] = test['freq'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errori = test[(test['freq'] > 1)]\n",
    "\n",
    "# test = test[(test['freq'] < 1)]\n",
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errori = errori.groupby(['url_aat', 'offsetInContext', 'misspelledWord']).first()\n",
    "# errori_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "errori.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doubles = df_doubles[(df_doubles['freq'] > 1)]\n",
    "# df_doubles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = list(set(untupled['person'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = df_error_backup[(~df_error_backup['misspelledWord'].isin(false_positives)) & \n",
    "                         (df_error_backup['language'] == 'en')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest['misspelledWord'].value_counts().iloc[40:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dftest[(dftest['language'] == 'en')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## Filter out cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spacies[(df_spacies['language'] != 'en')].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_holland = df_error_backup[(df_error_backup['sentence'].str.contains('Holland') == True)]\n",
    "\n",
    "# # creates human readable ulan link, based on lod link\n",
    "# df_holland['url_ulan'] = df_holland['x.value'].apply(create_ulan_weblink)\n",
    "\n",
    "# the one errouneous holland\n",
    "# df_holland = df_holland[(df_holland['url_ulan'].str.contains('500256837') == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cols to allign with other cols\n",
    "df_holland['ruleId'] = 'PREFERRED SPELLING'\n",
    "df_holland['message'] = 'Prefered spelling is the Netherlands.'\n",
    "df_holland['replacements'] = ['The Netherlands']\n",
    "df_holland['offsetInContext'] = 0\n",
    "df_holland['context'] = df_holland['bio.value']\n",
    "df_holland['offset'] = 0\n",
    "df_holland['errorLength'] = 0\n",
    "df_holland['category'] = 'PREFERRED SPELLING'\n",
    "df_holland['ruleIssueType'] = 'typographical'\n",
    "df_holland['sentence'] = df_holland['bio.value']\n",
    "df_holland['misspelledWord'] = 'Holland'\n",
    "\n",
    "# rename cols\n",
    "df_holland = df_holland.rename(columns={'x.value':'url'})\n",
    "\n",
    "# select cols\n",
    "df_holland = df_holland[['url', 'ruleId', 'message', 'replacements', 'offsetInContext',\n",
    "       'context', 'offset', 'errorLength', 'category', 'ruleIssueType',\n",
    "       'sentence', 'misspelledWord', 'url_ulan']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "# Merge all found errors and create export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "df_spell_check = pd.concat([df_checked_errors, df_punctuation, df_holland])\n",
    "\n",
    "# select cols\n",
    "df_spell_check = df_spell_check[['url', \n",
    "                                 'url_ulan', \n",
    "                                'sentence',\n",
    "                                 'ruleId', \n",
    "                                 'message', \n",
    "                                 'replacements', \n",
    "                                 'offsetInContext',\n",
    "#                                  'context', \n",
    "                                 'offset', \n",
    "                                 'errorLength', \n",
    "                                 'category', \n",
    "                                 'ruleIssueType',\n",
    "                                 'misspelledWord'\n",
    "                                ]]\n",
    "\n",
    "# rename cols\n",
    "df_spell_check = df_spell_check.rename(columns={'url' : 'url_lod'})\n",
    "\n",
    "# reset index\n",
    "df_spell_check = df_spell_check.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_excel_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spell_check.to_excel('data_dumps\\\\' + filename_excel_export, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "# Check for similar names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52631291/vectorizing-or-speeding-up-fuzzywuzzy-string-matching-on-pandas-column\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# df = pd.DataFrame([['cliftonlarsonallen llp minneapolis MN'],\n",
    "#         ['loeb and troper llp newyork NY'],\n",
    "#         [\"dauby o'connor and zaleski llc carmel IN\"],\n",
    "#         ['wegner cpas llp madison WI']],\n",
    "#         columns=['org_name'])\n",
    "\n",
    "name_vals = results_df['name.value'].to_list()\n",
    "\n",
    "# name_vals = name_vals[0:10]\n",
    "\n",
    "threshold = 90\n",
    "\n",
    "def find_match(x):\n",
    "    ''''''\n",
    "    match = process.extract(x, name_vals, limit=2, scorer=fuzz.partial_token_sort_ratio)\n",
    "#     match = match if match[1] > threshold else np.nan\n",
    "    return match\n",
    "\n",
    "# results_df['match_found'] = results_df['name.value'].progress_apply(find_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# create list\n",
    "name_vals = results_df['name.value'].to_list()\n",
    "\n",
    "name_vals = name_vals[0:5]\n",
    "\n",
    "#Create tuples of brand names, matched brand names, and the score\n",
    "score_sort = [(x,) + i\n",
    "             for x in tqdm(name_vals)\n",
    "             for i in process.extract(x, name_vals, scorer=fuzz.token_sort_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe from the tuples\n",
    "df_similarity = pd.DataFrame(score_sort, columns=['artist','match_sort','similarity_score'])\n",
    "\n",
    "# df_similarity = df_similarity[(df_similarity['score_sort'] > 91) &\n",
    "#                               (df_similarity['score_sort'] != 100)]\n",
    "\n",
    "# # create back up filename for a pickle\n",
    "# time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "# filename_df_similarity = f'{time_stamp}_df_similarity.xlsx'\n",
    "\n",
    "# # export\n",
    "# df_similarity.to_excel('data_dumps\\\\' + filename_df_similarity, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "# Other options for spell-checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['IA', 'KS', 'UT', 'VA', 'NC', 'NE', 'SD', 'AL', 'ID', 'FM', 'DE', 'AK', 'CT', 'PR', 'NM', 'MS', 'PW', 'CO', 'NJ', 'FL', 'MN', 'VI', 'NV', 'AZ', 'WI', 'ND', 'PA', 'OK', 'KY', 'RI', 'NH', 'MO', 'ME', 'VT', 'GA', 'GU', 'AS', 'NY', 'CA', 'HI', 'IL', 'TN', 'MA', 'OH', 'MD', 'MI', 'WY', 'WA', 'OR', 'MH', 'SC', 'IN', 'LA', 'MP', 'DC', 'MT', 'AR', 'WV', 'TX']\n",
    "regex = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "states2 = ['I.A.', 'K.S.', 'U.T.', 'V.A.', 'N.C.', 'N.E.', 'S.D.', 'A.L.', 'I.D.', 'F.M.', 'D.E.', 'A.K.', 'C.T.',\n",
    "           'P.R.', 'N.M.', 'M.S.', 'P.W.', 'C.O.', 'N.J.', 'F.L.', 'M.N.', 'V.I.', 'N.V.', 'A.Z.', 'W.I.', 'N.D.', \n",
    "           'P.A.', 'O.K.', 'K.Y.', 'R.I.', 'N.H.', 'M.O.', 'M.E.', 'V.T.', 'G.A.', 'G.U.', 'A.S.', 'N.Y.', 'C.A.', \n",
    "           'H.I.', 'I.L.', 'T.N.', 'M.A.', 'O.H.', 'M.D.', 'M.I.', 'W.Y.', 'W.A.', 'O.R.', 'M.H.', 'S.C.', 'I.N.', \n",
    "           'L.A.', 'M.P.', 'D.C.', 'M.T.', 'A.R.', 'W.V.', 'T.X.']\n",
    "regex2 = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_finder(string:str) -> list:\n",
    "    ''''''\n",
    "    states2 = []\n",
    "#     states = ['IA', 'KS', 'UT', 'VA', 'NC', 'NE', 'SD', 'AL', 'ID', 'FM', 'DE', 'AK', 'CT', 'PR', 'NM', 'MS', 'PW', \n",
    "#               'CO', 'NJ', 'FL', 'MN', 'VI', 'NV', 'AZ', 'WI', 'ND', 'PA', 'OK', 'KY', 'RI', 'NH', 'MO', 'ME', 'VT', \n",
    "#               'GA', 'GU', 'AS', 'NY', 'CA', 'HI', 'IL', 'TN', 'MA', 'OH', 'MD', 'MI', 'WY', 'WA', 'OR', 'MH', 'SC', \n",
    "#               'IN', 'LA', 'MP', 'DC', 'MT', 'AR', 'WV', 'TX']\n",
    "\n",
    "    states = ['I.A.', 'K.S.', 'U.T.', 'V.A.', 'N.C.', 'N.E.', 'S.D.', 'A.L.', 'I.D.', 'F.M.', 'D.E.', 'A.K.', 'C.T.',\n",
    "           'P.R.', 'N.M.', 'M.S.', 'P.W.', 'C.O.', 'N.J.', 'F.L.', 'M.N.', 'V.I.', 'N.V.', 'A.Z.', 'W.I.', 'N.D.', \n",
    "           'P.A.', 'O.K.', 'K.Y.', 'R.I.', 'N.H.', 'M.O.', 'M.E.', 'V.T.', 'G.A.', 'G.U.', 'A.S.', 'N.Y.', 'C.A.', \n",
    "           'H.I.', 'I.L.', 'T.N.', 'M.A.', 'O.H.', 'M.D.', 'M.I.', 'W.Y.', 'W.A.', 'O.R.', 'M.H.', 'S.C.', 'I.N.', \n",
    "           'L.A.', 'M.P.', 'D.C.', 'M.T.', 'A.R.', 'W.V.', 'T.X.']                   \n",
    "                  \n",
    "    regex = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        string = str(string)\n",
    "        states2 = re.findall(regex , string)\n",
    "        return states2\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tqdm.pandas(desc=\"power DataFrame 1M to 100 random int!\")\n",
    "results_df['test'] = results_df['bio.value'].progress_apply(state_finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df['test'].str.len() > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\n",
    "#     (results_df['bio.value'].str.contains('I.A.') == True) |\n",
    "    (results_df['bio.value'].str.contains('Calif\\.') == True) |\n",
    "    (results_df['bio.value'].str.contains('CA') == True) |\n",
    "    (results_df['bio.value'].str.contains('C\\.A\\.') == True) |\n",
    "    (results_df['bio.value'].str.contains('California') == True)]\n",
    "# (results_df['bio.value'].str.contains('K.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('U.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('S.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('F.M.') == True) |\n",
    "# (results_df['bio.value'].str.contains('D.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.K.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.M.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.W.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.O.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.J.') == True) |\n",
    "# (results_df['bio.value'].str.contains('F.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.V.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.Z.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.K.') == True) |\n",
    "# (results_df['bio.value'].str.contains('K.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('R.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.O.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('G.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('G.U.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('H.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('T.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('S.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('L.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.P.') == True) |\n",
    "# (results_df['bio.value'].str.contains('D.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.V.') == True) |\n",
    "# (results_df['bio.value'].str.contains('T.X.') == True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\n",
    "#     (results_df\n",
    "#             ['bio.value'].str.contains('Alabama') == True) | \n",
    "# (results_df['bio.value'].str.contains('Alaska') == True) | \n",
    "# (results_df['bio.value'].str.contains('Arizona') == True) | \n",
    "# (results_df['bio.value'].str.contains('Arkansas') == True) | \n",
    "# (results_df['bio.value'].str.contains('California') == True) | \n",
    "# (results_df['bio.value'].str.contains('Colorado') == True) | \n",
    "# (results_df['bio.value'].str.contains('Connecticut') == True) | \n",
    "# (results_df['bio.value'].str.contains('Delaware') == True) | \n",
    "# (results_df['bio.value'].str.contains('Florida') == True) | \n",
    "# (results_df['bio.value'].str.contains('Georgia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Hawaii') == True) | \n",
    "# (results_df['bio.value'].str.contains('Idaho') == True) | \n",
    "# (results_df['bio.value'].str.contains('Illinois') == True) | \n",
    "# (results_df['bio.value'].str.contains('Indiana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Iowa') == True) | \n",
    "# (results_df['bio.value'].str.contains('Kansas') == True) | \n",
    "# (results_df['bio.value'].str.contains('Kentucky') == True) | \n",
    "# (results_df['bio.value'].str.contains('Louisiana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Maine') == True) | \n",
    "# (results_df['bio.value'].str.contains('Maryland') == True) | \n",
    "# (results_df['bio.value'].str.contains('Massachusetts') == True) | \n",
    "# (results_df['bio.value'].str.contains('Michigan') == True) | \n",
    "# (results_df['bio.value'].str.contains('Minnesota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Mississippi') == True) | \n",
    "# (results_df['bio.value'].str.contains('Missouri') == True) | \n",
    "# (results_df['bio.value'].str.contains('Montana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Nebraska') == True) | \n",
    "# (results_df['bio.value'].str.contains('Nevada') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Hampshire') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Jersey') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Mexico') == True) | \n",
    "# (results_df['bio.value'].str.contains('New York') == True) | \n",
    "# (results_df['bio.value'].str.contains('North Carolina') == True) | \n",
    "# (results_df['bio.value'].str.contains('North Dakota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Ohio') == True) | \n",
    "# (results_df['bio.value'].str.contains('Oklahoma') == True) | \n",
    "# (results_df['bio.value'].str.contains('Oregon') == True) | \n",
    "# (results_df['bio.value'].str.contains('Pennsylvania') == True) | \n",
    "# (results_df['bio.value'].str.contains('Rhode Island') == True) | \n",
    "# (results_df['bio.value'].str.contains('South Carolina') == True) | \n",
    "# (results_df['bio.value'].str.contains('South Dakota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Tennessee') == True) | \n",
    "# (results_df['bio.value'].str.contains('Texas') == True) | \n",
    "# (results_df['bio.value'].str.contains('Utah') == True) | \n",
    "# (results_df['bio.value'].str.contains('Vermont') == True) | \n",
    "# (results_df['bio.value'].str.contains('Virginia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Washington') == True) | \n",
    "# (results_df['bio.value'].str.contains('West Virginia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Wisconsin') == True) | \n",
    "# (results_df['bio.value'].str.contains('Wyoming') == True) | \n",
    "# (results_df['bio.value'].str.contains('District of Columbia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Guam') == True) | \n",
    "# (results_df['bio.value'].str.contains('Marshall Islands') == True) | \n",
    "# (results_df['bio.value'].str.contains('Northern Mariana Island') == True) | \n",
    "# (results_df['bio.value'].str.contains('Puerto Rico') == True) | \n",
    "# (results_df['bio.value'].str.contains('Virgin Islands') == True) | \n",
    "# (results_df['bio.value'].str.contains('AL') == True) | \n",
    "# (results_df['bio.value'].str.contains('AK') == True) | \n",
    "# (results_df['bio.value'].str.contains('AZ') == True) | \n",
    "# (results_df['bio.value'].str.contains('AR') == True) | \n",
    "# (results_df['bio.value'].str.contains('CA') == True) | \n",
    "# (results_df['bio.value'].str.contains('CO') == True) | \n",
    "# (results_df['bio.value'].str.contains('CT') == True) | \n",
    "# (results_df['bio.value'].str.contains('DE') == True) | \n",
    "# (results_df['bio.value'].str.contains('FL') == True) | \n",
    "# (results_df['bio.value'].str.contains('GA') == True) | \n",
    "# (results_df['bio.value'].str.contains('HI') == True) | \n",
    "# (results_df['bio.value'].str.contains('ID') == True) | \n",
    "# (results_df['bio.value'].str.contains('IL') == True) | \n",
    "# (results_df['bio.value'].str.contains('IN') == True) | \n",
    "# (results_df['bio.value'].str.contains('IA') == True) | \n",
    "# (results_df['bio.value'].str.contains('KS') == True) | \n",
    "# (results_df['bio.value'].str.contains('KY') == True) | \n",
    "# (results_df['bio.value'].str.contains('LA') == True) | \n",
    "# (results_df['bio.value'].str.contains('ME') == True) | \n",
    "# (results_df['bio.value'].str.contains('MD') == True) | \n",
    "# (results_df['bio.value'].str.contains('MA') == True) | \n",
    "# (results_df['bio.value'].str.contains('MI') == True) | \n",
    "# (results_df['bio.value'].str.contains('MN') == True) | \n",
    "# (results_df['bio.value'].str.contains('MS') == True) | \n",
    "# (results_df['bio.value'].str.contains('MO') == True) | \n",
    "# (results_df['bio.value'].str.contains('MT') == True) | \n",
    "# (results_df['bio.value'].str.contains('NE') == True) | \n",
    "# (results_df['bio.value'].str.contains('NV') == True) | \n",
    "# (results_df['bio.value'].str.contains('NH') == True) | \n",
    "# (results_df['bio.value'].str.contains('NJ') == True) | \n",
    "# (results_df['bio.value'].str.contains('NM') == True) | \n",
    "# (results_df['bio.value'].str.contains('NY') == True) | \n",
    "# (results_df['bio.value'].str.contains('NC') == True) | \n",
    "# (results_df['bio.value'].str.contains('ND') == True) | \n",
    "# (results_df['bio.value'].str.contains('OH') == True) | \n",
    "# (results_df['bio.value'].str.contains('OK') == True) | \n",
    "# (results_df['bio.value'].str.contains('OR') == True) | \n",
    "# (results_df['bio.value'].str.contains('PA') == True) | \n",
    "# (results_df['bio.value'].str.contains('RI') == True) | \n",
    "# (results_df['bio.value'].str.contains('SC') == True) | \n",
    "# (results_df['bio.value'].str.contains('SD') == True) | \n",
    "# (results_df['bio.value'].str.contains('TN') == True) | \n",
    "(results_df['bio.value'].str.contains('TX') == True) | \n",
    "(results_df['bio.value'].str.contains('T.X.') == True) | \n",
    "# (results_df['bio.value'].str.contains('UT') == True) | \n",
    "# (results_df['bio.value'].str.contains('VT') == True) | \n",
    "# (results_df['bio.value'].str.contains('VA') == True) | \n",
    "# (results_df['bio.value'].str.contains('WA') == True) | \n",
    "# (results_df['bio.value'].str.contains('WV') == True) | \n",
    "# (results_df['bio.value'].str.contains('WI') == True) | \n",
    "# (results_df['bio.value'].str.contains('WY') == True) | \n",
    "# (results_df['bio.value'].str.contains('DC') == True) | \n",
    "# (results_df['bio.value'].str.contains('GU') == True) | \n",
    "# (results_df['bio.value'].str.contains('MH') == True) | \n",
    "# (results_df['bio.value'].str.contains('MP') == True) | \n",
    "# (results_df['bio.value'].str.contains('PR') == True) | \n",
    "# (results_df['bio.value'].str.contains('VI') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ala.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Alaska') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ariz.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ark.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Calif.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Color.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Conn.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Del.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Fla.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ga.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Hawaii') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Idaho') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ill.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ind.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Iowa') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Kan.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ky.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' La.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Maine') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Md.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mass.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mich.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Minn.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Miss.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mo.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mont.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Neb.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Nev.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.H.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.J.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.M.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.Y.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.D.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ohio') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Okla.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ore.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Pa.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' R.I.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' S.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' S.Dak.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Tenn.') == True) | \n",
    "(results_df['bio.value'].str.contains(' Tex.') == True)  ]\n",
    "# (results_df['bio.value'].str.contains(' Utah') == True) | \n",
    "# (results_df['bio.value'].str.contains(' V.T.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Va.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wash.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' W.Va.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wis.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wyo.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' D.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Guam') == True) | \n",
    "# (results_df['bio.value'].str.contains(' M.I.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' CNMI') == True) | \n",
    "# (results_df['bio.value'].str.contains(' P.R. or PUR') == True) | \n",
    "# (results_df['bio.value'].str.contains(' V.I.') == True) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['test'] = results_df['bio.value'].str.extract(r'(?!BC|CE)(A-Z)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df['test'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df['bio.value'].str.contains('Tex') == True)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_upper['misspelledWord'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_upper['misspelledWord'].iloc[26:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors['test'] = df_errors['context'].astype(str).str.extract('([\\s]+-[\\s]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors[(df_errors['test'].notnull())].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
