{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7729f4b",
   "metadata": {},
   "source": [
    "# AAT data check | 300264086 = Associated Concepts Facet\n",
    "- 2024-05-18\n",
    "- Language check on Getty ULAN data acquired via SPARQL-endpoint\n",
    "- V. Martens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556680f8",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc987dd6",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6526eb61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:46:21.709907Z",
     "start_time": "2024-05-31T09:46:20.037391Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handling jsons\n",
    "import json\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# spell check module\n",
    "import language_tool_python\n",
    "\n",
    "# creating time stamps\n",
    "# from datetime import datetime\n",
    "import time\n",
    "\n",
    "# importing files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# regex module\n",
    "import re\n",
    "\n",
    "# for multi-threading\n",
    "from concurrent import futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# back up files\n",
    "import pickle\n",
    "\n",
    "# check for western spellings\n",
    "from alphabet_detector import AlphabetDetector\n",
    "\n",
    "# spacy lang detect\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "from neg_filter_aat import *\n",
    "\n",
    "# preferences\n",
    "# adjust pandas to show all cols\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397d7e4",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391b0cb",
   "metadata": {},
   "source": [
    "### LOD results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be21c169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:46:23.089180Z",
     "start_time": "2024-05-31T09:46:23.050994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12983, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load lod-query from getty_ulan_sparql_endpoint script\n",
    "%store -r results_df\n",
    "\n",
    "# to create a more standardized naming\n",
    "df_results = results_df.copy()\n",
    "\n",
    "results_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699844e2",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e15a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:46:24.035799Z",
     "start_time": "2024-05-31T09:46:24.029104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240602-111025_df_errors_aat.pickle, 20240602-111025_df_lod_results_aat.pickle, 20240602-111025_spell_check_AAT_300264086.xlsx\n"
     ]
    }
   ],
   "source": [
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_df_errors = f'{time_stamp}_df_errors_aat.pickle'\n",
    "\n",
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_df_lod_results = f'{time_stamp}_df_lod_results_aat.pickle'\n",
    "\n",
    "# create back up filename for a pickle\n",
    "time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "filename_excel_export = f'{time_stamp}_spell_check_AAT_300264086.xlsx'\n",
    "\n",
    "print(f\"{filename_df_errors}, {filename_df_lod_results}, {filename_excel_export}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ddfdd",
   "metadata": {},
   "source": [
    "## Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a83d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:18:45.431089Z",
     "start_time": "2024-05-31T12:18:44.940665Z"
    }
   },
   "outputs": [],
   "source": [
    "@Language.factory('language_detector')\n",
    "def language_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "def check_df_row(df, col, i):\n",
    "    '''\n",
    "    '''\n",
    "    dict_source_matches = {}\n",
    "    try:\n",
    "        matches = tool.check(df[col].iloc[i])\n",
    "        dict_source_matches[df['Subject.value'].iloc[i]] = matches \n",
    "        return dict_source_matches\n",
    "    except (JSONDecodeError, NameError) as e:\n",
    "        return dict_source_matches[df['Subject.value'].iloc[i]] == 'na'\n",
    "        print(e, df['Subject.value'].iloc[i])\n",
    "    \n",
    "    \n",
    "def parse_list_of_jsons(json_list: list) -> pd.DataFrame():\n",
    "    '''doc string'''\n",
    "    df_errors = pd.DataFrame()\n",
    "\n",
    "    for item in json_list:\n",
    "\n",
    "        for k, v in item.items():\n",
    "            source = k \n",
    "\n",
    "            for item in (v):\n",
    "\n",
    "                df_error = pd.DataFrame(\n",
    "                    (\n",
    "                             source,\n",
    "                             item.ruleId,\n",
    "                             item.message,\n",
    "                             item.replacements,\n",
    "                             item.offsetInContext,\n",
    "                             item.context,\n",
    "                             item.offset,\n",
    "                             item.errorLength,\n",
    "                             item.category,\n",
    "                             item.ruleIssueType,\n",
    "                             item.sentence,\n",
    "                             item.context[item.offsetInContext:int(item.offsetInContext + item.errorLength)],\n",
    "                            )\n",
    "                ).T\n",
    "\n",
    "                df_errors = pd.concat([df_errors, df_error])\n",
    "\n",
    "    df_errors = df_errors.rename(columns={\n",
    "              0 : \"url\",\n",
    "              1 : 'ruleId',\n",
    "              2 : 'message', \n",
    "              3 : 'replacements',\n",
    "              4 : 'offsetInContext',\n",
    "              5 : 'context',\n",
    "              6 : 'offset', \n",
    "              7 : 'errorLength',\n",
    "              8 : 'category', \n",
    "              9 : 'ruleIssueType', \n",
    "              10 : 'sentence',\n",
    "              11 : 'misspelledWord'})          \n",
    "\n",
    "    return df_errors\n",
    "\n",
    "def load_latest_file(filepath:str) -> str:\n",
    "    '''args: string with filepath\n",
    "    returns: latest file'''\n",
    "    list_of_files = glob.glob(filepath)\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    return latest_file\n",
    "\n",
    "def create_aat_weblink(string:str) -> str:\n",
    "    '''from a lod-url creates a regular ulan webpage link\n",
    "    args: ulan lod landing page\n",
    "    returns: regular human readable webpage\n",
    "    '''\n",
    "    aat_regex = re.compile('\\d+')\n",
    "    match_class_object = re.search(aat_regex, string)\n",
    "    aat_id = match_class_object.group(0)\n",
    "    return f\"https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid={aat_id}\"\n",
    "\n",
    "def check_for_western_chars(string):\n",
    "    try:\n",
    "        valids = re.match(r\"[A-Za-z-]+\", string).group(0)\n",
    "        return valids\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def check_for_capitalization(string):\n",
    "    try:\n",
    "        valids = re.match(r\"^[A-Z]\", string).group(0)\n",
    "        return valids\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99bc6d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7458a478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T07:28:57.494515Z",
     "start_time": "2024-05-31T07:28:57.487120Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count = mp.cpu_count()\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e266348e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:47:31.115777Z",
     "start_time": "2024-05-31T06:47:31.103797Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[311], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results_df \u001b[38;5;241m=\u001b[39m results_df[(results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScopeNote.value\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull())]\n\u001b[0;32m      2\u001b[0m results_df\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "results_df = results_df[(results_df['ScopeNote.value'].notnull())]\n",
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe2717d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:47:31.513359Z",
     "start_time": "2024-05-31T06:47:31.492468Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject.type</th>\n",
       "      <th>Subject.value</th>\n",
       "      <th>ScopeNote.xml:lang</th>\n",
       "      <th>ScopeNote.type</th>\n",
       "      <th>ScopeNote.value</th>\n",
       "      <th>Term.xml:lang</th>\n",
       "      <th>Term.type</th>\n",
       "      <th>Term.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300189559</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Referring to the sex that in reproduction normally produces sperm cells or male gametes.</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300189557</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Referring to the sex that normally produces eggs or female germ cells.</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300451703</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Motility or other conditions that limit a person's movements, senses, or activities in a conventional sense.</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>physical disabilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300266528</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Persian wool carpets made in Herat, characterized by a wine red field color, and a border of clear emerald green and asymmetrical knotting; often made of silk.</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Herat carpets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://vocab.getty.edu/aat/300056330</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>Pictorial narrative device featuring two or more successive actions from the same story or related stories within one setting. Term may be confused with \"simultaneous representation,\" which has to do with point of view, not with sequential episodes depicted in one scene.</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>continuous narration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject.type                         Subject.value ScopeNote.xml:lang  \\\n",
       "0          uri  http://vocab.getty.edu/aat/300189559                 en   \n",
       "1          uri  http://vocab.getty.edu/aat/300189557                 en   \n",
       "2          uri  http://vocab.getty.edu/aat/300451703                 en   \n",
       "3          uri  http://vocab.getty.edu/aat/300266528                 en   \n",
       "4          uri  http://vocab.getty.edu/aat/300056330                 en   \n",
       "\n",
       "  ScopeNote.type  \\\n",
       "0        literal   \n",
       "1        literal   \n",
       "2        literal   \n",
       "3        literal   \n",
       "4        literal   \n",
       "\n",
       "                                                                                                                                                                                                                                                                   ScopeNote.value  \\\n",
       "0                                                                                                                                                                                         Referring to the sex that in reproduction normally produces sperm cells or male gametes.   \n",
       "1                                                                                                                                                                                                           Referring to the sex that normally produces eggs or female germ cells.   \n",
       "2                                                                                                                                                                     Motility or other conditions that limit a person's movements, senses, or activities in a conventional sense.   \n",
       "3                                                                                                                 Persian wool carpets made in Herat, characterized by a wine red field color, and a border of clear emerald green and asymmetrical knotting; often made of silk.    \n",
       "4  Pictorial narrative device featuring two or more successive actions from the same story or related stories within one setting. Term may be confused with \"simultaneous representation,\" which has to do with point of view, not with sequential episodes depicted in one scene.   \n",
       "\n",
       "  Term.xml:lang Term.type             Term.value  \n",
       "0            en   literal                   male  \n",
       "1            en   literal                 female  \n",
       "2            en   literal  physical disabilities  \n",
       "3            en   literal          Herat carpets  \n",
       "4            en   literal   continuous narration  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "05d7f416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T20:44:29.613135Z",
     "start_time": "2024-05-30T20:18:17.550976Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cpu_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:7\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cpu_count' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# instance of language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Start pool\n",
    "thread_pool = ThreadPoolExecutor(max_workers=cpu_count, thread_name_prefix = 'Thread')\n",
    "\n",
    "# reate futures\n",
    "futures = [thread_pool.submit(check_df_row, results_df, 'ScopeNote.value', i) for i in tqdm(range(len(results_df)), total=len(results_df), desc='building futures')]\n",
    "\n",
    "# submit tasks\n",
    "results = [future.result() for future in tqdm(futures, total=len(futures), desc='spell check data')]\n",
    "\n",
    "# # Changed the func to add a df[col], to make it more compatible\n",
    "# # instance of language_tool_python\n",
    "# tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# # Start pool\n",
    "# thread_pool = ThreadPoolExecutor(max_workers=cpu_count, thread_name_prefix = 'Thread')\n",
    "\n",
    "# # reate futures\n",
    "# futures = [thread_pool.submit(check_df_row, results_df, i) for i in tqdm(range(len(results_df)) , total=len(results_df), desc='building futures')]\n",
    "\n",
    "# # submit tasks\n",
    "# results = [future.result() for future in tqdm(futures, total=len(futures), desc='spell check data')]\n",
    "\n",
    "# results = []\n",
    "# for future in tqdm(futures, total=len(futures), desc='spell check data'):\n",
    "#     try:\n",
    "#         results.append(future.result())\n",
    "#     except (JSONDecodeError, NameError) as e:\n",
    "#         print(e)\n",
    "#         pass\n",
    "\n",
    "# # close tool\n",
    "# tool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf996f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T20:45:41.344728Z",
     "start_time": "2024-05-30T20:45:36.437212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_errors = parse_list_of_jsons(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a5f7d",
   "metadata": {},
   "source": [
    "# Back up as a pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555f594",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##  Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab78cb23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T07:22:06.085141Z",
     "start_time": "2024-05-31T07:22:05.285073Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_errors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_dumps/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename_df_errors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m----> 2\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(df_errors, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_dumps/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename_df_lod_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m      5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(results_df, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_errors' is not defined"
     ]
    }
   ],
   "source": [
    "with open(f'data_dumps/{filename_df_errors}', 'wb') as handle:\n",
    "    pickle.dump(df_errors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f'data_dumps/{filename_df_lod_results}', 'wb') as handle:\n",
    "    pickle.dump(results_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aad596",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f438edc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:46:27.121468Z",
     "start_time": "2024-05-31T09:46:27.086889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dumps\\20240530-221803_df_lod_results_aat.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10242, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*lod_results_aat.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_lod_results_backup = pickle.load(file)\n",
    "    \n",
    "df_lod_results_backup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9627c006",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:46:28.589170Z",
     "start_time": "2024-05-31T09:46:28.546371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dumps\\20240530-221803_df_errors_aat.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5759, 12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_picke_file = load_latest_file('data_dumps/*errors_aat.pickle')\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(latest_picke_file, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    df_error_backup = pickle.load(file)\n",
    "    \n",
    "df_error_backup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e459763",
   "metadata": {},
   "source": [
    "# Manipulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef395f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:46:29.478042Z",
     "start_time": "2024-05-31T09:46:29.443135Z"
    }
   },
   "outputs": [],
   "source": [
    "# creates human readable ulan link, based on lod link\n",
    "df_error_backup['url_aat'] = df_error_backup['url'].apply(create_aat_weblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c1742",
   "metadata": {},
   "source": [
    "# Filter out spelling mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e61f28",
   "metadata": {},
   "source": [
    "## Subsetting based on ruleId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8988491c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:46:31.293542Z",
     "start_time": "2024-05-31T09:46:31.277284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar mistakes: 80\n",
      " agreement errors: 3 \n",
      " punctuation mistakes 159 \n",
      " other errors: 5501 \n",
      " floors: 13 \n",
      " random finds: 2\n"
     ]
    }
   ],
   "source": [
    "# filter out punctuation errors\n",
    "df_punctuation = df_error_backup[(df_error_backup['ruleId'] == 'UPPERCASE_SENTENCE_START') | \n",
    "                                 (df_error_backup['ruleId'] == 'COMMA_PARENTHESIS_WHITESPACE') |\n",
    "                                 (df_error_backup['ruleId'] == 'WHITESPACE_RULE') | \n",
    "                                 (df_error_backup['ruleId'].str.contains('EN_COMPOUNDS') == True) | \n",
    "                                 (df_error_backup['ruleId'].str.contains('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA') == True) | \n",
    "                                 (df_error_backup['ruleId'].str.contains('MISSING_COMMA_AFTER_INTRODUCTORY_PHRASE') == True) | \n",
    "                                 (df_error_backup['ruleId'] == 'SENTENCE_WHITESPACE' ) | \n",
    "                                 (df_error_backup['ruleId'] == 'ETC_PERIOD') | \n",
    "                                 (df_error_backup['ruleId'] == 'LC_AFTER_PERIOD') | # punctuation\n",
    "                                 (df_error_backup['ruleId'] == 'EN_DIACRITICS_REPLACE_DECOLLETAGE') | # \n",
    "                                 (df_error_backup['ruleId'] == 'EN_DIACRITICS_REPLACE_CREPE') |  # punctuation\n",
    "                                 (df_error_backup['ruleId'] == 'FILE_EXTENSIONS_CASE')] # caps\n",
    "\n",
    "\n",
    "df_agreement = df_error_backup[((df_error_backup['ruleId'] == 'AGREEMENT_SENT_START') & (df_error_backup['misspelledWord'] == 'remains')) |\n",
    "                               ((df_error_backup['ruleId'] == 'AGREEMENT_SENT_START') & (df_error_backup['misspelledWord'] == 'locks')) |\n",
    "                               ((df_error_backup['ruleId'] == 'AGREEMENT_SENT_START') & (df_error_backup['misspelledWord'] == 'is'))]\n",
    "\n",
    "# filter out grammatical errors\n",
    "df_grammar = df_error_backup[(df_error_backup['ruleId'].str.contains('DEPEND_ON') == True) |\n",
    "                             (df_error_backup['ruleId'].str.contains('FEWER_LESS') == True) |\n",
    "                             (df_error_backup['ruleId'].str.contains('ENGLISH_WORD_REPEAT_BEGINNING_RULE') == True) |\n",
    "                             (df_error_backup['ruleId'].str.contains('EN_DIACRITICS_REPLACE_CREPE') == True) |\n",
    "                             (df_error_backup['ruleId'] == 'FOR_FRO') |\n",
    "                             (df_error_backup['ruleId'] == 'SUBJECT_VERB_AGREEMENT_PLURAL') |\n",
    "                             (df_error_backup['ruleId'] == 'NODT_DOZEN') |\n",
    "                             (df_error_backup['ruleId'] == 'TO_NON_BASE') |\n",
    "                             (df_error_backup['ruleId'] == 'AS_IS_VBG') |\n",
    "                             (df_error_backup['ruleId'] == 'MD_BASEFORM') |\n",
    "                             (df_error_backup['ruleId'] == 'THROUGH_THOROUGH') | # not a false hit, but needs to be throughflow, instead of 'through flow'\n",
    "                             (df_error_backup['ruleId'] == 'MANY_KINDS_OF') |\n",
    "                             (df_error_backup['ruleId'] == 'BE_VBP_IN') |\n",
    "                             (df_error_backup['ruleId'] == 'AFTERWARDS_US') |\n",
    "                             (df_error_backup['ruleId'] == 'NNS_THAT_VBZ') |\n",
    "                             (df_error_backup['ruleId'] == 'PLURAL_MODIFIER') |\n",
    "                             (df_error_backup['ruleId'] == 'PRP_PAST_PART') |\n",
    "                             (df_error_backup['ruleId'] == 'EVERYDAY_EVERY_DAY') |\n",
    "                             (df_error_backup['ruleId'] == 'ENGLISH_WORD_REPEAT_BEGINNING_RULE') |\n",
    "                             (df_error_backup['ruleId'] == 'FOR_FRO') |\n",
    "                             (df_error_backup['ruleId'] == 'SUBJECT_VERB_AGREEMENT_PLURAL') |\n",
    "                             (df_error_backup['ruleId'] == 'ARTICLE_ADJECTIVE_OF') |\n",
    "                             (df_error_backup['ruleId'] == 'EN_A_VS_AN') |\n",
    "                             (df_error_backup['ruleId'] == 'POSSESSIVE_APOSTROPHE') |\n",
    "                             (df_error_backup['ruleId'] == 'THE_SUPERLATIVE') |\n",
    "                             (df_error_backup['ruleId'] == 'PLURAL_THAT_AGREEMENT')]\n",
    "\n",
    "# subsetting on false hits, and historical left-over\n",
    "df_error_backup = df_error_backup[(df_error_backup['ruleId'] != 'UPPERCASE_SENTENCE_START') &\n",
    "                                  (df_error_backup['ruleId'] != 'COMMA_PARENTHESIS_WHITESPACE') & \n",
    "                                  (df_error_backup['ruleId'] != 'WHITESPACE_RULE') &\n",
    "                                  (df_error_backup['ruleId'].str.contains('EN_COMPOUNDS') == False) & \n",
    "                                  (df_error_backup['ruleId'].str.contains('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA') == False) &\n",
    "                                  (df_error_backup['ruleId'].str.contains('MISSING_COMMA_AFTER_INTRODUCTORY_PHRASE') == False) & \n",
    "                                  (df_error_backup['ruleId'] != 'SENTENCE_WHITESPACE' ) &\n",
    "                                  (df_error_backup['ruleId'].str.contains('DEPEND_ON') == False) &\n",
    "                                  (df_error_backup['ruleId'] != 'ETC_PERIOD') & \n",
    "                                  (df_error_backup['ruleId'] != 'LC_AFTER_PERIOD') & # punctuation\n",
    "                                  (df_error_backup['ruleId'] != 'EN_DIACRITICS_REPLACE_DECOLLETAGE') & # punctuation\n",
    "                                  (df_error_backup['ruleId'] != 'EN_DIACRITICS_REPLACE_CREPE') & # punctuation\n",
    "                                  (df_error_backup['ruleId'] != 'FILE_EXTENSIONS_CASE') & # caps\n",
    "                                  (df_error_backup['ruleId'].str.contains('RED_READ') == False) & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'RITE_WRITE') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'FROM_FORM') &  # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'BESIDES_BESIDE') &  # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'NEITHER_NOR') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'OTHER_OTHERS') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'ITS_TO_IT_S') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'IT_IS_JJ_TO_VBG') & # false hit\n",
    "                                  (df_error_backup['ruleId'] != 'CANT') & \n",
    "                                  (df_error_backup['ruleId'] != 'AGREEMENT_SENT_START' )] # false hit\n",
    "\n",
    "# randomly found errors\n",
    "df_random_finds =  df_error_backup[(df_error_backup['misspelledWord'] == 'Scandanavian') |\n",
    "                                   (df_error_backup['misspelledWord'] == 'Piemontese')]\n",
    "\n",
    "df_floor = df_error_backup[(df_error_backup['misspelledWord'] == 'storey' ) |  # floor in us english\n",
    "                           (df_error_backup['misspelledWord'] == 'storeys' )]\n",
    "\n",
    "\n",
    "# if ruleId not in df_other_errors and not in df_random_finds\n",
    "df_error_backup = df_error_backup[(~df_error_backup['ruleId'].isin(df_grammar['ruleId'])) & \n",
    "                                  (~df_error_backup['ruleId'].isin(df_punctuation['ruleId']))]\n",
    "\n",
    "df_error_backup = df_error_backup[(~df_error_backup['misspelledWord'].isin(df_random_finds['misspelledWord']))]\n",
    "\n",
    "print(f\"grammar mistakes: {df_grammar.shape[0]}\\n agreement errors: {df_agreement.shape[0]} \\n punctuation mistakes {df_punctuation.shape[0]} \\n other errors: {df_error_backup.shape[0]} \\n floors: {df_floor.shape[0]} \\n random finds: {df_random_finds.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dd58a",
   "metadata": {},
   "source": [
    "# Filter out foreign hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66c76abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other errors: 4386, \n",
      " foreign errors: 116\n"
     ]
    }
   ],
   "source": [
    "# filters out sentences with most likely foreign words \n",
    "df_foreign = df_error_backup[(df_error_backup['sentence'].str.lower().str.contains('indian') == True) |\n",
    "                             (df_error_backup['sentence'].str.lower().str.contains('arabic') == True) |\n",
    "                             (df_error_backup['sentence'].str.lower().str.contains('chinese') == True) |\n",
    "                             (df_error_backup['sentence'].str.lower().str.contains('persian') == True) |\n",
    "                             (df_error_backup['sentence'].str.lower().str.contains('nahuatl') == True) ]\n",
    "\n",
    "# checked subset for hits and subset those into a df\n",
    "df_foreign_positive = df_foreign[(df_foreign['misspelledWord'].str.contains('mould') == True) | \n",
    "                                 (df_foreign['misspelledWord'].str.contains('storey') == True) |\n",
    "                                 (df_foreign['misspelledWord'] == 'doorframe') |\n",
    "                                 (df_foreign['misspelledWord'] == 'coffered') |\n",
    "                                 (df_foreign['misspelledWord'] == 'doorsill') |\n",
    "                                 (df_foreign['misspelledWord'] == 'oversailing') |\n",
    "                                 (df_foreign['misspelledWord'] == 'atlantid') |\n",
    "                                 (df_foreign['misspelledWord'] == '\\'')]\n",
    "\n",
    "# filters out sentences with most likely foreign words\n",
    "df_error_backup = df_error_backup[(df_error_backup['sentence'].str.lower().str.contains('indian') == False) & \n",
    "                                  (df_error_backup['sentence'].str.lower().str.contains('arabic') == False) &\n",
    "                                  (df_error_backup['sentence'].str.lower().str.contains('chinese') == False) &\n",
    "                                  (df_error_backup['sentence'].str.lower().str.contains('persian') == False) & \n",
    "                                  (df_error_backup['sentence'].str.lower().str.contains('nahuatl') == False)]\n",
    "\n",
    "print(f\"other errors: {df_error_backup.shape[0]}, \\n foreign errors: {df_foreign_positive.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00d020",
   "metadata": {},
   "source": [
    "## Filter out frequently occuring mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec90b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other errors: 2531\n"
     ]
    }
   ],
   "source": [
    "df_error_backup = negative_aat_filter(df_error_backup)\n",
    "print(f\"other errors: {df_error_backup.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1aedd0",
   "metadata": {},
   "source": [
    "## Filter out words that start with a capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e784e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other errors: 1254, \n",
      " 23\n"
     ]
    }
   ],
   "source": [
    "# run regex func\n",
    "df_error_backup['capital_start'] = df_error_backup['misspelledWord'].apply(check_for_capitalization)\n",
    "\n",
    "df_capitals = df_error_backup[(df_error_backup['capital_start'].notnull())]\n",
    "\n",
    "df_capitals_positive = df_capitals[(df_capitals['misspelledWord'] == 'Bookbindings' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'Web site' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'Catholic church' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'Eastern Orthodox church' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'It dark' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'With the exception of' ) |\n",
    "                                   (df_capitals['misspelledWord'] == 'Finno-Ugric' ) | \n",
    "                                   (df_capitals['misspelledWord'] == 'Syrian desert' ) ]\n",
    "\n",
    "df_error_backup = df_error_backup[(df_error_backup['capital_start'].isnull())]\n",
    "del df_error_backup['capital_start']\n",
    "\n",
    "print(f\"other errors: {df_error_backup.shape[0]}, \\n {df_capitals_positive.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53d2ea",
   "metadata": {},
   "source": [
    "# Check for language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09c79ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other errors: 546\n"
     ]
    }
   ],
   "source": [
    "# subsets chars from non western chars\n",
    "df_error_backup['western_chars'] = df_error_backup['misspelledWord'].apply(check_for_western_chars)\n",
    "\n",
    "# see if length is equal otherwise drop non-western chars\n",
    "df_error_backup = df_error_backup[(df_error_backup['western_chars'].str.len() == df_error_backup['misspelledWord'].str.len())]\n",
    "\n",
    "print(f\"other errors: {df_error_backup.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee06ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:56:20.663726Z",
     "start_time": "2024-05-31T06:56:20.659693Z"
    }
   },
   "source": [
    "## Check for non-Western typescripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ca0b37d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:46:32.364529Z",
     "start_time": "2024-05-31T09:46:32.306816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-western chars: 0\n",
      "other errors: 546\n"
     ]
    }
   ],
   "source": [
    "ad = AlphabetDetector()\n",
    "df_error_backup['western_typo'] = df_error_backup['misspelledWord'].apply(lambda x: ad.is_latin(x))\n",
    "\n",
    "print(f\"non-western chars: {df_error_backup[(df_error_backup['western_typo'] == False )].shape[0]}\")\n",
    "df_error_backup = df_error_backup[(df_error_backup['western_typo'] == True )]\n",
    "\n",
    "del df_error_backup['western_typo']\n",
    "\n",
    "print(f\"other errors: {df_error_backup.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "f2a8ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = pd.DataFrame(df_error_backup['misspelledWord'].value_counts().iloc[95:100]).reset_index()\n",
    "# list_error = x['misspelledWord'].to_list()\n",
    "# print(list_error)\n",
    "\n",
    "# df_error_backup[(df_error_backup['misspelledWord'].isin(list_error))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdf6e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in manual:\n",
    "    # print(f\"                (df_error_backup['misspelledWord'] != '{item}' ) &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8d157",
   "metadata": {},
   "source": [
    "## Lang detect Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "4d8f68df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:17:13.457808Z",
     "start_time": "2024-05-31T12:15:25.463848Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1381/1381 [00:28<00:00, 48.23it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 2000000\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "# initiate master df\n",
    "df_spacies = pd.DataFrame()\n",
    "\n",
    "# run over words\n",
    "for item in tqdm(df_error_backup['misspelledWord'].tolist()):\n",
    "    \n",
    "    # store data in doc\n",
    "    doc = nlp(str(item))\n",
    "    detect_language = doc._.language\n",
    "\n",
    "    # cast and store results into variable\n",
    "    word = str(doc)\n",
    "    language = str(detect_language['language'])\n",
    "    score =  float(detect_language['score'])\n",
    "\n",
    "    # add to df\n",
    "    df_spacy = pd.DataFrame((word, language, score)).T\n",
    "    df_spacies = pd.concat([df_spacy, df_spacies])\n",
    "\n",
    "# manipulate all spacy results\n",
    "df_spacies = (df_spacies\n",
    "             .rename(columns={0: 'word',\n",
    "                              1: 'language',\n",
    "                              2: 'score'})\n",
    "             .sort_values('score', ascending=False)\n",
    "             .groupby(by=['word', 'language'])\n",
    "             .first()\n",
    "             .reset_index())\n",
    "\n",
    "# subset false hits\n",
    "df_spacies = df_spacies[(df_spacies['word'].str.len() > 3)]\n",
    "\n",
    "# merge back with main df\n",
    "df_error_backup = pd.merge(df_error_backup, \n",
    "                           df_spacies, \n",
    "                           left_on='misspelledWord', \n",
    "                           right_on='word', \n",
    "                           how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e8bba",
   "metadata": {},
   "source": [
    "# Check for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c72ae07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:30:35.476636Z",
     "start_time": "2024-05-31T12:30:02.562822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75d86beb3ef48c89954dbabf9c9747f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run spacy on trained name data\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "entity_list = []\n",
    "\n",
    "# run over annotations\n",
    "for i in tqdm(df_error_backup['misspelledWord'].tolist()):\n",
    "    # store data in doc\n",
    "    doc = nlp(str(i))\n",
    "    \n",
    "    # retrieve entities from doc and ad it to a list\n",
    "    for entity in doc.ents:\n",
    "#         print(entity.text, entity.label_)\n",
    "        entity_list.append((i, entity.text, entity.label_))\n",
    "    \n",
    "# untuple the list    \n",
    "untupled = pd.DataFrame([[y for y in  x] for x in entity_list])\n",
    "\n",
    "# add col names\n",
    "untupled = untupled.rename(columns={0:'word',\n",
    "                                    1:'spacy_word',\n",
    "                                    2:'entity_label'})\n",
    "\n",
    "# merge back with main df\n",
    "df_error_backup = pd.merge(df_error_backup, \n",
    "                           untupled, \n",
    "                           left_on='misspelledWord', \n",
    "                           right_on='word', \n",
    "                           how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef76d7",
   "metadata": {},
   "source": [
    "# Merge all errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb65e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors found: 966\n"
     ]
    }
   ],
   "source": [
    "# merge\n",
    "df_spell_check = pd.concat([df_agreement, df_error_backup, df_grammar, df_punctuation, df_random_finds, df_floor, df_foreign_positive, df_capitals_positive])\n",
    "\n",
    "# select cols\n",
    "df_spell_check = df_spell_check[['url', \n",
    "                                 'url_aat', \n",
    "                                 'ruleId', \n",
    "                                 'message', \n",
    "                                 'sentence',\n",
    "                                 'misspelledWord',\n",
    "                                 'replacements', \n",
    "                                #  'offsetInContext',\n",
    "#                                  'context', \n",
    "                                 'offset', \n",
    "                                 'errorLength', \n",
    "                                 'category', \n",
    "                                 'ruleIssueType',\n",
    "                                ]]\n",
    "\n",
    "# df_spell_check.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "# rename cols\n",
    "df_spell_check = df_spell_check.rename(columns={'url' : 'url_lod_aat'})\n",
    "\n",
    "# reset index\n",
    "df_spell_check = df_spell_check.reset_index().drop(columns='index')\n",
    "\n",
    "print(f\"errors found: {df_spell_check.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09d1cb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_lod_aat</th>\n",
       "      <th>url_aat</th>\n",
       "      <th>ruleId</th>\n",
       "      <th>message</th>\n",
       "      <th>sentence</th>\n",
       "      <th>misspelledWord</th>\n",
       "      <th>replacements</th>\n",
       "      <th>offset</th>\n",
       "      <th>errorLength</th>\n",
       "      <th>category</th>\n",
       "      <th>ruleIssueType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://vocab.getty.edu/aat/300264460</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300264460</td>\n",
       "      <td>AGREEMENT_SENT_START</td>\n",
       "      <td>You should probably use “remain”.</td>\n",
       "      <td>Both factions remains powerful and popular today and have their main temples in Kyoto.</td>\n",
       "      <td>remains</td>\n",
       "      <td>[remain]</td>\n",
       "      <td>798</td>\n",
       "      <td>7</td>\n",
       "      <td>GRAMMAR</td>\n",
       "      <td>grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://vocab.getty.edu/aat/300225150</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300225150</td>\n",
       "      <td>AGREEMENT_SENT_START</td>\n",
       "      <td>You should probably use “lock”.</td>\n",
       "      <td>Those parts of firearm locks that engage the hammer to hold it at full or half cock, until released by the trigger.</td>\n",
       "      <td>locks</td>\n",
       "      <td>[lock]</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>GRAMMAR</td>\n",
       "      <td>grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://vocab.getty.edu/aat/300265218</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300265218</td>\n",
       "      <td>AGREEMENT_SENT_START</td>\n",
       "      <td>You should probably use: “are”.</td>\n",
       "      <td>The terms is sometimes translated as flavor, mood, or aesthetic delight.</td>\n",
       "      <td>is</td>\n",
       "      <td>[are]</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>GRAMMAR</td>\n",
       "      <td>grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://vocab.getty.edu/aat/300451409</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300451409</td>\n",
       "      <td>MORFOLOGIK_RULE_EN_US</td>\n",
       "      <td>Possible spelling mistake found.</td>\n",
       "      <td>Time that is measured by direct observation of the Sun, by a sundial, astrolab, or directly by another device.</td>\n",
       "      <td>astrolab</td>\n",
       "      <td>[astrolabe]</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>TYPOS</td>\n",
       "      <td>misspelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://vocab.getty.edu/aat/300446753</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300446753</td>\n",
       "      <td>MORFOLOGIK_RULE_EN_US</td>\n",
       "      <td>Possible spelling mistake. ‘moulding’ is British English.</td>\n",
       "      <td>A type of kaṇṭha, or recessed moulding; recessed moulding in adhiṣṭhāna.</td>\n",
       "      <td>moulding</td>\n",
       "      <td>[molding]</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>TYPOS</td>\n",
       "      <td>misspelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>http://vocab.getty.edu/aat/300417781</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300417781</td>\n",
       "      <td>MORFOLOGIK_RULE_EN_US</td>\n",
       "      <td>Possible spelling mistake found.</td>\n",
       "      <td>Bookbindings having raised letters or designs on the surface.</td>\n",
       "      <td>Bookbindings</td>\n",
       "      <td>[Bookbinding, Book bindings]</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>TYPOS</td>\n",
       "      <td>misspelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>http://vocab.getty.edu/aat/300400855</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300400855</td>\n",
       "      <td>MORFOLOGIK_RULE_EN_US</td>\n",
       "      <td>Possible spelling mistake found.</td>\n",
       "      <td>Pre-Christian and pre-Islamic religious beliefs and practices of the Finno-Ugric peoples, who inhabit regions of northern Scandinavia, Siberia, the Baltic area, and central Europe.</td>\n",
       "      <td>Finno-Ugric</td>\n",
       "      <td>[Finn-Ugric, Finno-Uric]</td>\n",
       "      <td>69</td>\n",
       "      <td>11</td>\n",
       "      <td>TYPOS</td>\n",
       "      <td>misspelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>http://vocab.getty.edu/aat/300417756</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300417756</td>\n",
       "      <td>MORFOLOGIK_RULE_EN_US</td>\n",
       "      <td>Possible spelling mistake found.</td>\n",
       "      <td>Bookbindings that use leather, generally either as the primary cover or to wrap a board cover.</td>\n",
       "      <td>Bookbindings</td>\n",
       "      <td>[Bookbinding, Book bindings]</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>TYPOS</td>\n",
       "      <td>misspelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>http://vocab.getty.edu/aat/300417741</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300417741</td>\n",
       "      <td>MORFOLOGIK_RULE_EN_US</td>\n",
       "      <td>Possible spelling mistake found.</td>\n",
       "      <td>Bookbindings decorated with painting, often along with gilding or other decoration.</td>\n",
       "      <td>Bookbindings</td>\n",
       "      <td>[Bookbinding, Book bindings]</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>TYPOS</td>\n",
       "      <td>misspelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>http://vocab.getty.edu/aat/300391521</td>\n",
       "      <td>https://www.getty.edu/vow/AATFullDisplay?find=&amp;logic=AND&amp;note=&amp;subjectid=300391521</td>\n",
       "      <td>MORFOLOGIK_RULE_EN_US</td>\n",
       "      <td>Possible spelling mistake found.</td>\n",
       "      <td>Moribund language of the Finnic division of Finno-Ugric languages; closely related to Estonian.</td>\n",
       "      <td>Finno-Ugric</td>\n",
       "      <td>[Finn-Ugric, Finno-Uric]</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>TYPOS</td>\n",
       "      <td>misspelling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>966 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              url_lod_aat  \\\n",
       "0    http://vocab.getty.edu/aat/300264460   \n",
       "1    http://vocab.getty.edu/aat/300225150   \n",
       "2    http://vocab.getty.edu/aat/300265218   \n",
       "3    http://vocab.getty.edu/aat/300451409   \n",
       "4    http://vocab.getty.edu/aat/300446753   \n",
       "..                                    ...   \n",
       "961  http://vocab.getty.edu/aat/300417781   \n",
       "962  http://vocab.getty.edu/aat/300400855   \n",
       "963  http://vocab.getty.edu/aat/300417756   \n",
       "964  http://vocab.getty.edu/aat/300417741   \n",
       "965  http://vocab.getty.edu/aat/300391521   \n",
       "\n",
       "                                                                                url_aat  \\\n",
       "0    https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300264460   \n",
       "1    https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300225150   \n",
       "2    https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300265218   \n",
       "3    https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300451409   \n",
       "4    https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300446753   \n",
       "..                                                                                  ...   \n",
       "961  https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300417781   \n",
       "962  https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300400855   \n",
       "963  https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300417756   \n",
       "964  https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300417741   \n",
       "965  https://www.getty.edu/vow/AATFullDisplay?find=&logic=AND&note=&subjectid=300391521   \n",
       "\n",
       "                    ruleId  \\\n",
       "0     AGREEMENT_SENT_START   \n",
       "1     AGREEMENT_SENT_START   \n",
       "2     AGREEMENT_SENT_START   \n",
       "3    MORFOLOGIK_RULE_EN_US   \n",
       "4    MORFOLOGIK_RULE_EN_US   \n",
       "..                     ...   \n",
       "961  MORFOLOGIK_RULE_EN_US   \n",
       "962  MORFOLOGIK_RULE_EN_US   \n",
       "963  MORFOLOGIK_RULE_EN_US   \n",
       "964  MORFOLOGIK_RULE_EN_US   \n",
       "965  MORFOLOGIK_RULE_EN_US   \n",
       "\n",
       "                                                       message  \\\n",
       "0                            You should probably use “remain”.   \n",
       "1                              You should probably use “lock”.   \n",
       "2                              You should probably use: “are”.   \n",
       "3                             Possible spelling mistake found.   \n",
       "4    Possible spelling mistake. ‘moulding’ is British English.   \n",
       "..                                                         ...   \n",
       "961                           Possible spelling mistake found.   \n",
       "962                           Possible spelling mistake found.   \n",
       "963                           Possible spelling mistake found.   \n",
       "964                           Possible spelling mistake found.   \n",
       "965                           Possible spelling mistake found.   \n",
       "\n",
       "                                                                                                                                                                                 sentence  \\\n",
       "0                                                                                                  Both factions remains powerful and popular today and have their main temples in Kyoto.   \n",
       "1                                                                     Those parts of firearm locks that engage the hammer to hold it at full or half cock, until released by the trigger.   \n",
       "2                                                                                                                The terms is sometimes translated as flavor, mood, or aesthetic delight.   \n",
       "3                                                                          Time that is measured by direct observation of the Sun, by a sundial, astrolab, or directly by another device.   \n",
       "4                                                                                                                A type of kaṇṭha, or recessed moulding; recessed moulding in adhiṣṭhāna.   \n",
       "..                                                                                                                                                                                    ...   \n",
       "961                                                                                                                         Bookbindings having raised letters or designs on the surface.   \n",
       "962  Pre-Christian and pre-Islamic religious beliefs and practices of the Finno-Ugric peoples, who inhabit regions of northern Scandinavia, Siberia, the Baltic area, and central Europe.   \n",
       "963                                                                                        Bookbindings that use leather, generally either as the primary cover or to wrap a board cover.   \n",
       "964                                                                                                   Bookbindings decorated with painting, often along with gilding or other decoration.   \n",
       "965                                                                                       Moribund language of the Finnic division of Finno-Ugric languages; closely related to Estonian.   \n",
       "\n",
       "    misspelledWord                  replacements offset errorLength category  \\\n",
       "0          remains                      [remain]    798           7  GRAMMAR   \n",
       "1            locks                        [lock]     23           5  GRAMMAR   \n",
       "2               is                         [are]    125           2  GRAMMAR   \n",
       "3         astrolab                   [astrolabe]     70           8    TYPOS   \n",
       "4         moulding                     [molding]     30           8    TYPOS   \n",
       "..             ...                           ...    ...         ...      ...   \n",
       "961   Bookbindings  [Bookbinding, Book bindings]      0          12    TYPOS   \n",
       "962    Finno-Ugric      [Finn-Ugric, Finno-Uric]     69          11    TYPOS   \n",
       "963   Bookbindings  [Bookbinding, Book bindings]      0          12    TYPOS   \n",
       "964   Bookbindings  [Bookbinding, Book bindings]      0          12    TYPOS   \n",
       "965    Finno-Ugric      [Finn-Ugric, Finno-Uric]     44          11    TYPOS   \n",
       "\n",
       "    ruleIssueType  \n",
       "0         grammar  \n",
       "1         grammar  \n",
       "2         grammar  \n",
       "3     misspelling  \n",
       "4     misspelling  \n",
       "..            ...  \n",
       "961   misspelling  \n",
       "962   misspelling  \n",
       "963   misspelling  \n",
       "964   misspelling  \n",
       "965   misspelling  \n",
       "\n",
       "[966 rows x 11 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spell_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea71f05",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "32c41ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spell_check.to_excel('data_dumps\\\\' + filename_excel_export, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "dd60c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doubles = df_spell_check.groupby(['url_aat', 'offsetInContext', 'misspelledWord']).size().reset_index().rename(columns={0 : 'freq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "48ec5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(df_spell_check, df_doubles, on=['url_aat', 'misspelledWord'], how='left')\n",
    "test['freq'] = test['freq'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "644459bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "73bd5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b3954e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errori = test[(test['freq'] > 1)]\n",
    "\n",
    "# test = test[(test['freq'] < 1)]\n",
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "89ef9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e0739216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errori = errori.groupby(['url_aat', 'offsetInContext', 'misspelledWord']).first()\n",
    "# errori_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "28e4e1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 10)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errori.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "6efa3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doubles = df_doubles[(df_doubles['freq'] > 1)]\n",
    "# df_doubles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a862a2",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "37595a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c4743379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:37:51.352834Z",
     "start_time": "2024-05-31T12:37:51.346598Z"
    }
   },
   "outputs": [],
   "source": [
    "false_positives = list(set(untupled['person'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "b2568e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:41:42.132929Z",
     "start_time": "2024-05-31T12:41:42.123028Z"
    }
   },
   "outputs": [],
   "source": [
    "dftest = df_error_backup[(~df_error_backup['misspelledWord'].isin(false_positives)) & \n",
    "                         (df_error_backup['language'] == 'en')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "bba8845a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:47:41.884512Z",
     "start_time": "2024-05-31T12:47:41.824525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207, 17)"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "17fb0079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:46:49.178350Z",
     "start_time": "2024-05-31T12:46:49.168805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "misspelledWord\n",
       "righ-hand           1\n",
       "chape               1\n",
       "orthogonals         1\n",
       "thorugh             1\n",
       "helically           1\n",
       "microform           1\n",
       "showerheads         1\n",
       "constucted          1\n",
       "overflowings        1\n",
       "aboveground         1\n",
       "pharmaceutically    1\n",
       "ploughing           1\n",
       "lowlying            1\n",
       "recyclability       1\n",
       "monitary            1\n",
       "ormamented          1\n",
       "seachannels         1\n",
       "fortications        1\n",
       "fourescent          1\n",
       "cracks              1\n",
       "stylograph          1\n",
       "hardwearing         1\n",
       "activties           1\n",
       "rushlights          1\n",
       "shoreward           1\n",
       "behaviour           1\n",
       "mouthes             1\n",
       "thetan              1\n",
       "undissected         1\n",
       "characers           1\n",
       "freemasons          1\n",
       "thaler              1\n",
       "screwheads          1\n",
       "synthesising        1\n",
       "associatively       1\n",
       "inclosed            1\n",
       "preisthood          1\n",
       "lockforming         1\n",
       "countershed         1\n",
       "heros               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest['misspelledWord'].value_counts().iloc[40:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35b8ebf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:25:17.871178Z",
     "start_time": "2024-05-31T12:25:17.829372Z"
    }
   },
   "outputs": [],
   "source": [
    "# dftest[(dftest['language'] == 'en')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2d075",
   "metadata": {},
   "source": [
    "## Filter out cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "9c1c97ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T12:05:41.762857Z",
     "start_time": "2024-05-31T12:05:41.745551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>applicaiton</td>\n",
       "      <td>it</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>lacemaking</td>\n",
       "      <td>tl</td>\n",
       "      <td>0.857137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>urens</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.857139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>globose</td>\n",
       "      <td>sl</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>petunidin</td>\n",
       "      <td>et</td>\n",
       "      <td>0.571426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>archivolts</td>\n",
       "      <td>it</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>uan-yang</td>\n",
       "      <td>id</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>arcadian</td>\n",
       "      <td>es</td>\n",
       "      <td>0.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>dvinity</td>\n",
       "      <td>lt</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>ghanta</td>\n",
       "      <td>id</td>\n",
       "      <td>0.707238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>cemetaries</td>\n",
       "      <td>ro</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>bimah</td>\n",
       "      <td>so</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>taipingliang</td>\n",
       "      <td>tl</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>cymas</td>\n",
       "      <td>cy</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>rondel</td>\n",
       "      <td>nl</td>\n",
       "      <td>0.571427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>arundinacea</td>\n",
       "      <td>ro</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>kai-amaiti</td>\n",
       "      <td>lt</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>fomer</td>\n",
       "      <td>da</td>\n",
       "      <td>0.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>sudatorium</td>\n",
       "      <td>ro</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>bannerstones</td>\n",
       "      <td>no</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>membraneous</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.714283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>lingam</td>\n",
       "      <td>tl</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>diglycerides</td>\n",
       "      <td>it</td>\n",
       "      <td>0.428572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>ectoderm</td>\n",
       "      <td>es</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>viariety</td>\n",
       "      <td>lt</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>bhitti-wall</td>\n",
       "      <td>sw</td>\n",
       "      <td>0.571426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>triratna</td>\n",
       "      <td>hr</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>palustris</td>\n",
       "      <td>et</td>\n",
       "      <td>0.713009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>windway</td>\n",
       "      <td>so</td>\n",
       "      <td>0.714282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>unwooded</td>\n",
       "      <td>nl</td>\n",
       "      <td>0.571426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>cyanoethylated</td>\n",
       "      <td>cy</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>nonspatial</td>\n",
       "      <td>it</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>pinrails</td>\n",
       "      <td>lv</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>kloosterkozijns</td>\n",
       "      <td>nl</td>\n",
       "      <td>0.571427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>ideogrammatic</td>\n",
       "      <td>it</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>footstalk</td>\n",
       "      <td>no</td>\n",
       "      <td>0.714282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>carophyllin</td>\n",
       "      <td>cy</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>primevil</td>\n",
       "      <td>sl</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>fluorochrome</td>\n",
       "      <td>sk</td>\n",
       "      <td>0.571427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>contrapposto</td>\n",
       "      <td>it</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>alphasyllabic</td>\n",
       "      <td>cy</td>\n",
       "      <td>0.714283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>strenghten</td>\n",
       "      <td>de</td>\n",
       "      <td>0.573959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>freesanding</td>\n",
       "      <td>af</td>\n",
       "      <td>0.714283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>polyptychs</td>\n",
       "      <td>pl</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>solariums</td>\n",
       "      <td>it</td>\n",
       "      <td>0.573156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>butsudan</td>\n",
       "      <td>id</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>bergamia</td>\n",
       "      <td>id</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>unsewn</td>\n",
       "      <td>de</td>\n",
       "      <td>0.571427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>bookrests</td>\n",
       "      <td>sk</td>\n",
       "      <td>0.571427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>noncontiguous</td>\n",
       "      <td>it</td>\n",
       "      <td>0.857141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word language     score\n",
       "55        applicaiton       it  0.999994\n",
       "584        lacemaking       tl  0.857137\n",
       "1134            urens       fr  0.857139\n",
       "456           globose       sl  0.999994\n",
       "786         petunidin       et  0.571426\n",
       "62         archivolts       it  0.999997\n",
       "1112         uan-yang       id  0.999997\n",
       "59           arcadian       es  0.999992\n",
       "367           dvinity       lt  0.999998\n",
       "452            ghanta       id  0.707238\n",
       "183        cemetaries       ro  0.999995\n",
       "120             bimah       so  0.999995\n",
       "1051     taipingliang       tl  0.999998\n",
       "279             cymas       cy  0.999998\n",
       "896            rondel       nl  0.571427\n",
       "73        arundinacea       ro  0.999997\n",
       "558        kai-amaiti       lt  0.999995\n",
       "423             fomer       da  0.999992\n",
       "1029       sudatorium       ro  0.999994\n",
       "92       bannerstones       no  0.999994\n",
       "641       membraneous       fr  0.714283\n",
       "603            lingam       tl  0.999995\n",
       "328      diglycerides       it  0.428572\n",
       "372          ectoderm       es  0.999993\n",
       "1146         viariety       lt  0.999996\n",
       "118       bhitti-wall       sw  0.571426\n",
       "1100         triratna       hr  0.999993\n",
       "760         palustris       et  0.713009\n",
       "1166          windway       so  0.714282\n",
       "1133         unwooded       nl  0.571426\n",
       "275    cyanoethylated       cy  0.999996\n",
       "714        nonspatial       it  0.999995\n",
       "798          pinrails       lv  0.999995\n",
       "574   kloosterkozijns       nl  0.571427\n",
       "508     ideogrammatic       it  0.999996\n",
       "424         footstalk       no  0.714282\n",
       "171       carophyllin       cy  0.999996\n",
       "840          primevil       sl  0.999996\n",
       "419      fluorochrome       sk  0.571427\n",
       "233      contrapposto       it  0.999997\n",
       "36      alphasyllabic       cy  0.714283\n",
       "1015       strenghten       de  0.573959\n",
       "432       freesanding       af  0.714283\n",
       "821        polyptychs       pl  0.999996\n",
       "990         solariums       it  0.573156\n",
       "155          butsudan       id  0.999996\n",
       "112          bergamia       id  0.999999\n",
       "1131           unsewn       de  0.571427\n",
       "135         bookrests       sk  0.571427\n",
       "707     noncontiguous       it  0.857141"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spacies[(df_spacies['language'] != 'en')].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "be38f218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:19.963806Z",
     "start_time": "2024-05-25T15:24:19.770892Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_holland = df_error_backup[(df_error_backup['sentence'].str.contains('Holland') == True)]\n",
    "\n",
    "# # creates human readable ulan link, based on lod link\n",
    "# df_holland['url_ulan'] = df_holland['x.value'].apply(create_ulan_weblink)\n",
    "\n",
    "# the one errouneous holland\n",
    "# df_holland = df_holland[(df_holland['url_ulan'].str.contains('500256837') == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96121ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:20.340798Z",
     "start_time": "2024-05-25T15:24:20.318778Z"
    }
   },
   "outputs": [],
   "source": [
    "# add cols to allign with other cols\n",
    "df_holland['ruleId'] = 'PREFERRED SPELLING'\n",
    "df_holland['message'] = 'Prefered spelling is the Netherlands.'\n",
    "df_holland['replacements'] = ['The Netherlands']\n",
    "df_holland['offsetInContext'] = 0\n",
    "df_holland['context'] = df_holland['bio.value']\n",
    "df_holland['offset'] = 0\n",
    "df_holland['errorLength'] = 0\n",
    "df_holland['category'] = 'PREFERRED SPELLING'\n",
    "df_holland['ruleIssueType'] = 'typographical'\n",
    "df_holland['sentence'] = df_holland['bio.value']\n",
    "df_holland['misspelledWord'] = 'Holland'\n",
    "\n",
    "# rename cols\n",
    "df_holland = df_holland.rename(columns={'x.value':'url'})\n",
    "\n",
    "# select cols\n",
    "df_holland = df_holland[['url', 'ruleId', 'message', 'replacements', 'offsetInContext',\n",
    "       'context', 'offset', 'errorLength', 'category', 'ruleIssueType',\n",
    "       'sentence', 'misspelledWord', 'url_ulan']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb4fb2",
   "metadata": {},
   "source": [
    "# Merge all found errors and create export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98f420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:21.459694Z",
     "start_time": "2024-05-25T15:24:21.446717Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge\n",
    "df_spell_check = pd.concat([df_checked_errors, df_punctuation, df_holland])\n",
    "\n",
    "# select cols\n",
    "df_spell_check = df_spell_check[['url', \n",
    "                                 'url_ulan', \n",
    "                                'sentence',\n",
    "                                 'ruleId', \n",
    "                                 'message', \n",
    "                                 'replacements', \n",
    "                                 'offsetInContext',\n",
    "#                                  'context', \n",
    "                                 'offset', \n",
    "                                 'errorLength', \n",
    "                                 'category', \n",
    "                                 'ruleIssueType',\n",
    "                                 'misspelledWord'\n",
    "                                ]]\n",
    "\n",
    "# rename cols\n",
    "df_spell_check = df_spell_check.rename(columns={'url' : 'url_lod'})\n",
    "\n",
    "# reset index\n",
    "df_spell_check = df_spell_check.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c38b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:24:22.056206Z",
     "start_time": "2024-05-25T15:24:22.048220Z"
    }
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad58935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:26:54.898285Z",
     "start_time": "2024-05-25T16:26:54.881329Z"
    }
   },
   "outputs": [],
   "source": [
    "filename_excel_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d90477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:29:55.144682Z",
     "start_time": "2024-05-25T16:29:54.699637Z"
    }
   },
   "outputs": [],
   "source": [
    "df_spell_check.to_excel('data_dumps\\\\' + filename_excel_export, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771b49f",
   "metadata": {},
   "source": [
    "# Check for similar names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3130a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:07:56.195951Z",
     "start_time": "2024-05-26T12:07:56.176999Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52631291/vectorizing-or-speeding-up-fuzzywuzzy-string-matching-on-pandas-column\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# df = pd.DataFrame([['cliftonlarsonallen llp minneapolis MN'],\n",
    "#         ['loeb and troper llp newyork NY'],\n",
    "#         [\"dauby o'connor and zaleski llc carmel IN\"],\n",
    "#         ['wegner cpas llp madison WI']],\n",
    "#         columns=['org_name'])\n",
    "\n",
    "name_vals = results_df['name.value'].to_list()\n",
    "\n",
    "# name_vals = name_vals[0:10]\n",
    "\n",
    "threshold = 90\n",
    "\n",
    "def find_match(x):\n",
    "    ''''''\n",
    "    match = process.extract(x, name_vals, limit=2, scorer=fuzz.partial_token_sort_ratio)\n",
    "#     match = match if match[1] > threshold else np.nan\n",
    "    return match\n",
    "\n",
    "# results_df['match_found'] = results_df['name.value'].progress_apply(find_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906590c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T13:13:06.383851Z",
     "start_time": "2024-05-26T13:13:06.357888Z"
    }
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc269d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T13:14:49.292745Z",
     "start_time": "2024-05-26T13:14:47.414889Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# create list\n",
    "name_vals = results_df['name.value'].to_list()\n",
    "\n",
    "name_vals = name_vals[0:5]\n",
    "\n",
    "#Create tuples of brand names, matched brand names, and the score\n",
    "score_sort = [(x,) + i\n",
    "             for x in tqdm(name_vals)\n",
    "             for i in process.extract(x, name_vals, scorer=fuzz.token_sort_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5734263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T13:14:53.582375Z",
     "start_time": "2024-05-26T13:14:53.554703Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a dataframe from the tuples\n",
    "df_similarity = pd.DataFrame(score_sort, columns=['artist','match_sort','similarity_score'])\n",
    "\n",
    "# df_similarity = df_similarity[(df_similarity['score_sort'] > 91) &\n",
    "#                               (df_similarity['score_sort'] != 100)]\n",
    "\n",
    "# # create back up filename for a pickle\n",
    "# time_stamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "# filename_df_similarity = f'{time_stamp}_df_similarity.xlsx'\n",
    "\n",
    "# # export\n",
    "# df_similarity.to_excel('data_dumps\\\\' + filename_df_similarity, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d346e5",
   "metadata": {},
   "source": [
    "# Other options for spell-checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee578f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T07:59:35.947908Z",
     "start_time": "2024-05-26T07:59:35.930455Z"
    }
   },
   "outputs": [],
   "source": [
    "states = ['IA', 'KS', 'UT', 'VA', 'NC', 'NE', 'SD', 'AL', 'ID', 'FM', 'DE', 'AK', 'CT', 'PR', 'NM', 'MS', 'PW', 'CO', 'NJ', 'FL', 'MN', 'VI', 'NV', 'AZ', 'WI', 'ND', 'PA', 'OK', 'KY', 'RI', 'NH', 'MO', 'ME', 'VT', 'GA', 'GU', 'AS', 'NY', 'CA', 'HI', 'IL', 'TN', 'MA', 'OH', 'MD', 'MI', 'WY', 'WA', 'OR', 'MH', 'SC', 'IN', 'LA', 'MP', 'DC', 'MT', 'AR', 'WV', 'TX']\n",
    "regex = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "states2 = ['I.A.', 'K.S.', 'U.T.', 'V.A.', 'N.C.', 'N.E.', 'S.D.', 'A.L.', 'I.D.', 'F.M.', 'D.E.', 'A.K.', 'C.T.',\n",
    "           'P.R.', 'N.M.', 'M.S.', 'P.W.', 'C.O.', 'N.J.', 'F.L.', 'M.N.', 'V.I.', 'N.V.', 'A.Z.', 'W.I.', 'N.D.', \n",
    "           'P.A.', 'O.K.', 'K.Y.', 'R.I.', 'N.H.', 'M.O.', 'M.E.', 'V.T.', 'G.A.', 'G.U.', 'A.S.', 'N.Y.', 'C.A.', \n",
    "           'H.I.', 'I.L.', 'T.N.', 'M.A.', 'O.H.', 'M.D.', 'M.I.', 'W.Y.', 'W.A.', 'O.R.', 'M.H.', 'S.C.', 'I.N.', \n",
    "           'L.A.', 'M.P.', 'D.C.', 'M.T.', 'A.R.', 'W.V.', 'T.X.']\n",
    "regex2 = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672dcb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:14:02.510455Z",
     "start_time": "2024-05-26T08:14:02.494470Z"
    }
   },
   "outputs": [],
   "source": [
    "def state_finder(string:str) -> list:\n",
    "    ''''''\n",
    "    states2 = []\n",
    "#     states = ['IA', 'KS', 'UT', 'VA', 'NC', 'NE', 'SD', 'AL', 'ID', 'FM', 'DE', 'AK', 'CT', 'PR', 'NM', 'MS', 'PW', \n",
    "#               'CO', 'NJ', 'FL', 'MN', 'VI', 'NV', 'AZ', 'WI', 'ND', 'PA', 'OK', 'KY', 'RI', 'NH', 'MO', 'ME', 'VT', \n",
    "#               'GA', 'GU', 'AS', 'NY', 'CA', 'HI', 'IL', 'TN', 'MA', 'OH', 'MD', 'MI', 'WY', 'WA', 'OR', 'MH', 'SC', \n",
    "#               'IN', 'LA', 'MP', 'DC', 'MT', 'AR', 'WV', 'TX']\n",
    "\n",
    "    states = ['I.A.', 'K.S.', 'U.T.', 'V.A.', 'N.C.', 'N.E.', 'S.D.', 'A.L.', 'I.D.', 'F.M.', 'D.E.', 'A.K.', 'C.T.',\n",
    "           'P.R.', 'N.M.', 'M.S.', 'P.W.', 'C.O.', 'N.J.', 'F.L.', 'M.N.', 'V.I.', 'N.V.', 'A.Z.', 'W.I.', 'N.D.', \n",
    "           'P.A.', 'O.K.', 'K.Y.', 'R.I.', 'N.H.', 'M.O.', 'M.E.', 'V.T.', 'G.A.', 'G.U.', 'A.S.', 'N.Y.', 'C.A.', \n",
    "           'H.I.', 'I.L.', 'T.N.', 'M.A.', 'O.H.', 'M.D.', 'M.I.', 'W.Y.', 'W.A.', 'O.R.', 'M.H.', 'S.C.', 'I.N.', \n",
    "           'L.A.', 'M.P.', 'D.C.', 'M.T.', 'A.R.', 'W.V.', 'T.X.']                   \n",
    "                  \n",
    "    regex = re.compile(r'\\b(' + '|'.join(states) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        string = str(string)\n",
    "        states2 = re.findall(regex , string)\n",
    "        return states2\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9ca58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:14:12.445318Z",
     "start_time": "2024-05-26T08:14:05.754724Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tqdm.pandas(desc=\"power DataFrame 1M to 100 random int!\")\n",
    "results_df['test'] = results_df['bio.value'].progress_apply(state_finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7edf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:14:13.336004Z",
     "start_time": "2024-05-26T08:14:13.176084Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[(results_df['test'].str.len() > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6e9f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:22:04.268925Z",
     "start_time": "2024-05-26T08:22:03.498290Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[\n",
    "#     (results_df['bio.value'].str.contains('I.A.') == True) |\n",
    "    (results_df['bio.value'].str.contains('Calif\\.') == True) |\n",
    "    (results_df['bio.value'].str.contains('CA') == True) |\n",
    "    (results_df['bio.value'].str.contains('C\\.A\\.') == True) |\n",
    "    (results_df['bio.value'].str.contains('California') == True)]\n",
    "# (results_df['bio.value'].str.contains('K.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('U.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('S.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('F.M.') == True) |\n",
    "# (results_df['bio.value'].str.contains('D.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.K.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.M.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.W.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.O.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.J.') == True) |\n",
    "# (results_df['bio.value'].str.contains('F.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.V.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.Z.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('P.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.K.') == True) |\n",
    "# (results_df['bio.value'].str.contains('K.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('R.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.O.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.E.') == True) |\n",
    "# (results_df['bio.value'].str.contains('V.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('G.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('G.U.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.S.') == True) |\n",
    "# (results_df['bio.value'].str.contains('N.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('C.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('H.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.L.') == True) |\n",
    "# (results_df['bio.value'].str.contains('T.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.D.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.I.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.Y.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('O.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.H.') == True) |\n",
    "# (results_df['bio.value'].str.contains('S.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('I.N.') == True) |\n",
    "# (results_df['bio.value'].str.contains('L.A.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.P.') == True) |\n",
    "# (results_df['bio.value'].str.contains('D.C.') == True) |\n",
    "# (results_df['bio.value'].str.contains('M.T.') == True) |\n",
    "# (results_df['bio.value'].str.contains('A.R.') == True) |\n",
    "# (results_df['bio.value'].str.contains('W.V.') == True) |\n",
    "# (results_df['bio.value'].str.contains('T.X.') == True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5a0fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T07:43:33.700146Z",
     "start_time": "2024-05-26T07:43:32.981319Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[\n",
    "#     (results_df\n",
    "#             ['bio.value'].str.contains('Alabama') == True) | \n",
    "# (results_df['bio.value'].str.contains('Alaska') == True) | \n",
    "# (results_df['bio.value'].str.contains('Arizona') == True) | \n",
    "# (results_df['bio.value'].str.contains('Arkansas') == True) | \n",
    "# (results_df['bio.value'].str.contains('California') == True) | \n",
    "# (results_df['bio.value'].str.contains('Colorado') == True) | \n",
    "# (results_df['bio.value'].str.contains('Connecticut') == True) | \n",
    "# (results_df['bio.value'].str.contains('Delaware') == True) | \n",
    "# (results_df['bio.value'].str.contains('Florida') == True) | \n",
    "# (results_df['bio.value'].str.contains('Georgia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Hawaii') == True) | \n",
    "# (results_df['bio.value'].str.contains('Idaho') == True) | \n",
    "# (results_df['bio.value'].str.contains('Illinois') == True) | \n",
    "# (results_df['bio.value'].str.contains('Indiana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Iowa') == True) | \n",
    "# (results_df['bio.value'].str.contains('Kansas') == True) | \n",
    "# (results_df['bio.value'].str.contains('Kentucky') == True) | \n",
    "# (results_df['bio.value'].str.contains('Louisiana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Maine') == True) | \n",
    "# (results_df['bio.value'].str.contains('Maryland') == True) | \n",
    "# (results_df['bio.value'].str.contains('Massachusetts') == True) | \n",
    "# (results_df['bio.value'].str.contains('Michigan') == True) | \n",
    "# (results_df['bio.value'].str.contains('Minnesota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Mississippi') == True) | \n",
    "# (results_df['bio.value'].str.contains('Missouri') == True) | \n",
    "# (results_df['bio.value'].str.contains('Montana') == True) | \n",
    "# (results_df['bio.value'].str.contains('Nebraska') == True) | \n",
    "# (results_df['bio.value'].str.contains('Nevada') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Hampshire') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Jersey') == True) | \n",
    "# (results_df['bio.value'].str.contains('New Mexico') == True) | \n",
    "# (results_df['bio.value'].str.contains('New York') == True) | \n",
    "# (results_df['bio.value'].str.contains('North Carolina') == True) | \n",
    "# (results_df['bio.value'].str.contains('North Dakota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Ohio') == True) | \n",
    "# (results_df['bio.value'].str.contains('Oklahoma') == True) | \n",
    "# (results_df['bio.value'].str.contains('Oregon') == True) | \n",
    "# (results_df['bio.value'].str.contains('Pennsylvania') == True) | \n",
    "# (results_df['bio.value'].str.contains('Rhode Island') == True) | \n",
    "# (results_df['bio.value'].str.contains('South Carolina') == True) | \n",
    "# (results_df['bio.value'].str.contains('South Dakota') == True) | \n",
    "# (results_df['bio.value'].str.contains('Tennessee') == True) | \n",
    "# (results_df['bio.value'].str.contains('Texas') == True) | \n",
    "# (results_df['bio.value'].str.contains('Utah') == True) | \n",
    "# (results_df['bio.value'].str.contains('Vermont') == True) | \n",
    "# (results_df['bio.value'].str.contains('Virginia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Washington') == True) | \n",
    "# (results_df['bio.value'].str.contains('West Virginia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Wisconsin') == True) | \n",
    "# (results_df['bio.value'].str.contains('Wyoming') == True) | \n",
    "# (results_df['bio.value'].str.contains('District of Columbia') == True) | \n",
    "# (results_df['bio.value'].str.contains('Guam') == True) | \n",
    "# (results_df['bio.value'].str.contains('Marshall Islands') == True) | \n",
    "# (results_df['bio.value'].str.contains('Northern Mariana Island') == True) | \n",
    "# (results_df['bio.value'].str.contains('Puerto Rico') == True) | \n",
    "# (results_df['bio.value'].str.contains('Virgin Islands') == True) | \n",
    "# (results_df['bio.value'].str.contains('AL') == True) | \n",
    "# (results_df['bio.value'].str.contains('AK') == True) | \n",
    "# (results_df['bio.value'].str.contains('AZ') == True) | \n",
    "# (results_df['bio.value'].str.contains('AR') == True) | \n",
    "# (results_df['bio.value'].str.contains('CA') == True) | \n",
    "# (results_df['bio.value'].str.contains('CO') == True) | \n",
    "# (results_df['bio.value'].str.contains('CT') == True) | \n",
    "# (results_df['bio.value'].str.contains('DE') == True) | \n",
    "# (results_df['bio.value'].str.contains('FL') == True) | \n",
    "# (results_df['bio.value'].str.contains('GA') == True) | \n",
    "# (results_df['bio.value'].str.contains('HI') == True) | \n",
    "# (results_df['bio.value'].str.contains('ID') == True) | \n",
    "# (results_df['bio.value'].str.contains('IL') == True) | \n",
    "# (results_df['bio.value'].str.contains('IN') == True) | \n",
    "# (results_df['bio.value'].str.contains('IA') == True) | \n",
    "# (results_df['bio.value'].str.contains('KS') == True) | \n",
    "# (results_df['bio.value'].str.contains('KY') == True) | \n",
    "# (results_df['bio.value'].str.contains('LA') == True) | \n",
    "# (results_df['bio.value'].str.contains('ME') == True) | \n",
    "# (results_df['bio.value'].str.contains('MD') == True) | \n",
    "# (results_df['bio.value'].str.contains('MA') == True) | \n",
    "# (results_df['bio.value'].str.contains('MI') == True) | \n",
    "# (results_df['bio.value'].str.contains('MN') == True) | \n",
    "# (results_df['bio.value'].str.contains('MS') == True) | \n",
    "# (results_df['bio.value'].str.contains('MO') == True) | \n",
    "# (results_df['bio.value'].str.contains('MT') == True) | \n",
    "# (results_df['bio.value'].str.contains('NE') == True) | \n",
    "# (results_df['bio.value'].str.contains('NV') == True) | \n",
    "# (results_df['bio.value'].str.contains('NH') == True) | \n",
    "# (results_df['bio.value'].str.contains('NJ') == True) | \n",
    "# (results_df['bio.value'].str.contains('NM') == True) | \n",
    "# (results_df['bio.value'].str.contains('NY') == True) | \n",
    "# (results_df['bio.value'].str.contains('NC') == True) | \n",
    "# (results_df['bio.value'].str.contains('ND') == True) | \n",
    "# (results_df['bio.value'].str.contains('OH') == True) | \n",
    "# (results_df['bio.value'].str.contains('OK') == True) | \n",
    "# (results_df['bio.value'].str.contains('OR') == True) | \n",
    "# (results_df['bio.value'].str.contains('PA') == True) | \n",
    "# (results_df['bio.value'].str.contains('RI') == True) | \n",
    "# (results_df['bio.value'].str.contains('SC') == True) | \n",
    "# (results_df['bio.value'].str.contains('SD') == True) | \n",
    "# (results_df['bio.value'].str.contains('TN') == True) | \n",
    "(results_df['bio.value'].str.contains('TX') == True) | \n",
    "(results_df['bio.value'].str.contains('T.X.') == True) | \n",
    "# (results_df['bio.value'].str.contains('UT') == True) | \n",
    "# (results_df['bio.value'].str.contains('VT') == True) | \n",
    "# (results_df['bio.value'].str.contains('VA') == True) | \n",
    "# (results_df['bio.value'].str.contains('WA') == True) | \n",
    "# (results_df['bio.value'].str.contains('WV') == True) | \n",
    "# (results_df['bio.value'].str.contains('WI') == True) | \n",
    "# (results_df['bio.value'].str.contains('WY') == True) | \n",
    "# (results_df['bio.value'].str.contains('DC') == True) | \n",
    "# (results_df['bio.value'].str.contains('GU') == True) | \n",
    "# (results_df['bio.value'].str.contains('MH') == True) | \n",
    "# (results_df['bio.value'].str.contains('MP') == True) | \n",
    "# (results_df['bio.value'].str.contains('PR') == True) | \n",
    "# (results_df['bio.value'].str.contains('VI') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ala.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Alaska') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ariz.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ark.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Calif.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Color.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Conn.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Del.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Fla.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ga.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Hawaii') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Idaho') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ill.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ind.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Iowa') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Kan.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ky.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' La.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Maine') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Md.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mass.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mich.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Minn.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Miss.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mo.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Mont.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Neb.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Nev.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.H.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.J.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.M.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.Y.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' N.D.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ohio') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Okla.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Ore.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Pa.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' R.I.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' S.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' S.Dak.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Tenn.') == True) | \n",
    "(results_df['bio.value'].str.contains(' Tex.') == True)  ]\n",
    "# (results_df['bio.value'].str.contains(' Utah') == True) | \n",
    "# (results_df['bio.value'].str.contains(' V.T.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Va.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wash.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' W.Va.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wis.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Wyo.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' D.C.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' Guam') == True) | \n",
    "# (results_df['bio.value'].str.contains(' M.I.') == True) | \n",
    "# (results_df['bio.value'].str.contains(' CNMI') == True) | \n",
    "# (results_df['bio.value'].str.contains(' P.R. or PUR') == True) | \n",
    "# (results_df['bio.value'].str.contains(' V.I.') == True) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283e1aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T07:53:21.650380Z",
     "start_time": "2024-05-26T07:53:20.890701Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df['test'] = results_df['bio.value'].str.extract(r'(?!BC|CE)(A-Z)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2f69f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T07:53:22.259406Z",
     "start_time": "2024-05-26T07:53:22.220232Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[(results_df['test'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b662c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:33:18.075638Z",
     "start_time": "2024-05-20T08:33:17.998025Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df[(results_df['bio.value'].str.contains('Tex') == True)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35ec04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:32:53.032231Z",
     "start_time": "2024-05-20T08:32:53.010159Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b28859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T17:12:39.650054Z",
     "start_time": "2024-05-19T17:12:39.636638Z"
    }
   },
   "outputs": [],
   "source": [
    "no_upper['misspelledWord'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695ae29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T16:48:40.530073Z",
     "start_time": "2024-05-19T16:48:40.513517Z"
    }
   },
   "outputs": [],
   "source": [
    "no_upper['misspelledWord'].iloc[26:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec4d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T12:42:54.471239Z",
     "start_time": "2024-05-19T12:42:54.452984Z"
    }
   },
   "outputs": [],
   "source": [
    "df_errors['test'] = df_errors['context'].astype(str).str.extract('([\\s]+-[\\s]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1873bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T12:47:13.861365Z",
     "start_time": "2024-05-19T12:47:13.830450Z"
    }
   },
   "outputs": [],
   "source": [
    "df_errors[(df_errors['test'].notnull())].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getty_ulan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
